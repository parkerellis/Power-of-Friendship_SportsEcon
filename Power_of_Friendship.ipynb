{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow5LPXQCdBjk"
      },
      "outputs": [],
      "source": [
        "w#Transcript Scraping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transcript Scraping Cell\n",
        "\n",
        "import requests, time, re, os, csv\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "BASE    = \"https://www.asapsports.com\"\n",
        "YEARS   = [2014, 2015, 2016, 2017, 2018, 2019]\n",
        "OUTPUT  = \"mlb_postseason_transcripts_2014_2019.csv\"\n",
        "OUTPATH = Path(OUTPUT)\n",
        "DELAY   = 0.3\n",
        "MIN_WORDS = 50\n",
        "\n",
        "# Include postseason; exclude non-MLB\n",
        "INCLUDE_PATTERNS = [\n",
        "    r\"\\bALDS\\b\", r\"\\bNLDS\\b\", r\"\\bALCS\\b\", r\"\\bNLCS\\b\",\n",
        "    r\"\\bDIVISION\\s+SERIES\\b\",\n",
        "    r\"\\bLEAGUE\\s+CHAMPIONSHIP\\s+SERIES\\b\",\n",
        "    r\"\\bCHAMPIONSHIP\\s+SERIES\\b\",\n",
        "    r\"\\bWORLD\\s+SERIES\\b\",\n",
        "]\n",
        "EXCLUDE_PATTERNS = [\n",
        "    r\"\\bCOLLEGE\\b\", r\"\\bNCAA\\b\", r\"\\bCOLLEGIATE\\b\",\n",
        "    r\"\\bWORLD\\s+BASEBALL\\s+CLASSIC\\b\", r\"\\bWBC\\b\",\n",
        "    r\"\\bLITTLE\\s+LEAGUE\\b\", r\"\\bALL[-\\s]?STAR\\b\", r\"\\bWINTER\\s+MEETINGS\\b\",\n",
        "    r\"\\bWILD[-\\s]?CARD\\b\", r\"\\bWILDCARD\\b\",\n",
        "]\n",
        "INCLUDE_RE = re.compile(\"|\".join(INCLUDE_PATTERNS), re.IGNORECASE)\n",
        "EXCLUDE_RE = re.compile(\"|\".join(EXCLUDE_PATTERNS), re.IGNORECASE)\n",
        "QA_MARKERS_RE = re.compile(r'(?:^|\\s)(Q[\\.\\:\\-\\u2013\\u2014])', re.IGNORECASE)\n",
        "\n",
        "def is_mlb_postseason(txt):\n",
        "    if not isinstance(txt, str): return False\n",
        "    if EXCLUDE_RE.search(txt):  return False\n",
        "    return bool(INCLUDE_RE.search(txt))\n",
        "\n",
        "def norm_url(href):\n",
        "    if href.startswith(\"http://\"): href = \"https://\" + href[7:]\n",
        "    if href.startswith(\"https://\"): return href\n",
        "    if href.startswith(\"/\"): return BASE + href\n",
        "    return BASE + \"/\" + href.lstrip(\"/\")\n",
        "\n",
        "def safe_get(sess, url, retries=2, sleep=1, timeout=18):\n",
        "    for _ in range(retries):\n",
        "        try:\n",
        "            r = sess.get(url, timeout=timeout)\n",
        "            if r.status_code == 200 and r.text:\n",
        "                return r.text\n",
        "        except Exception:\n",
        "            pass\n",
        "        time.sleep(sleep)\n",
        "    return None\n",
        "\n",
        "def clean_space(s): return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
        "\n",
        "def extract_transcript(soup):\n",
        "    # 1) paragraphs\n",
        "    ptxt = clean_space(\" \".join(p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")))\n",
        "    if len(ptxt.split()) >= MIN_WORDS: return ptxt\n",
        "    # 2) broader containers\n",
        "    collected = []\n",
        "    for tag in (\"div\",\"font\",\"span\",\"td\"):\n",
        "        for el in soup.find_all(tag):\n",
        "            t = el.get_text(\" \", strip=True)\n",
        "            if t: collected.append(t)\n",
        "    broad = clean_space(\" \".join(collected))\n",
        "    m = QA_MARKERS_RE.search(broad)\n",
        "    if m: broad = clean_space(broad[m.start():])\n",
        "    if len(broad.split()) >= MIN_WORDS: return broad\n",
        "    # 3) full body fallback\n",
        "    body = clean_space(soup.get_text(\" \", strip=True))\n",
        "    m2 = QA_MARKERS_RE.search(body)\n",
        "    if m2: body = clean_space(body[m2.start():])\n",
        "    return body\n",
        "\n",
        "def gather_interviews_on_page(sess, url):\n",
        "    html = safe_get(sess, url)\n",
        "    if not html: return [], (\"\",\"\")\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    title = soup.find(\"title\").get_text(\" \", strip=True) if soup.find(\"title\") else \"\"\n",
        "    body  = soup.get_text(\" \", strip=True)\n",
        "    links = [norm_url(a.get(\"href\")) for a in soup.select('a[href*=\"show_interview.php?id=\"]') if a.get(\"href\")]\n",
        "    if \"show_interview.php?id=\" in url: links.insert(0, url)\n",
        "    # dedupe\n",
        "    seen=set(); links=[u for u in links if not (u in seen or seen.add(u))]\n",
        "    return links, (title, body)\n",
        "\n",
        "# --- Streaming writer: create file early, maintain seen set for dedupe\n",
        "if not OUTPATH.exists():\n",
        "    with OUTPATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"year\",\"interview_url\",\"title\",\"word_count\",\"transcript\"])\n",
        "\n",
        "seen_urls = set()\n",
        "if OUTPATH.stat().st_size > 0:\n",
        "    try:\n",
        "        existing = pd.read_csv(OUTPATH, usecols=[\"interview_url\"])\n",
        "        seen_urls = set(existing[\"interview_url\"].dropna().tolist())\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def flush_rows(rows):\n",
        "    if not rows: return\n",
        "    with OUTPATH.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        for r in rows:\n",
        "            w.writerow([r[\"year\"], r[\"interview_url\"], r[\"title\"], r[\"word_count\"], r[\"transcript\"]])\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"Mozilla/5.0 (ASAP-MLB-Stream/1.0)\"})\n",
        "\n",
        "buffer = []\n",
        "per_year_added = {}\n",
        "\n",
        "for year in tqdm(YEARS, desc=\"Years\"):\n",
        "    start_added = 0\n",
        "\n",
        "    y_url = f\"{BASE}/show_year.php?category=2&year={year}\"\n",
        "    y_html = safe_get(session, y_url)\n",
        "    if not y_html:\n",
        "        print(f\"⚠️ No year page for {year}\")\n",
        "        continue\n",
        "    y_soup = BeautifulSoup(y_html, \"html.parser\")\n",
        "\n",
        "    # gather series and day pages (lenient at this stage)\n",
        "    series_pages = [norm_url(a[\"href\"]) for a in y_soup.select('a[href*=\"show_events.php?event_id=\"]')]\n",
        "    day_pages    = [norm_url(a[\"href\"]) for a in y_soup.select('a[href*=\"show_event.php?event_id=\"]')]\n",
        "    # dedupe\n",
        "    def dedupe(seq):\n",
        "        seen=set(); out=[]\n",
        "        for x in seq:\n",
        "            if x not in seen:\n",
        "                out.append(x); seen.add(x)\n",
        "        return out\n",
        "    series_pages = dedupe(series_pages)\n",
        "    day_pages    = dedupe(day_pages)\n",
        "\n",
        "    print(f\"{year}: {len(series_pages)} series pages, {len(day_pages)} day pages\")\n",
        "\n",
        "    # targets to crawl\n",
        "    targets = set(day_pages)\n",
        "    for sp in series_pages:\n",
        "        sp_html = safe_get(session, sp)\n",
        "        if not sp_html: continue\n",
        "        sp_soup = BeautifulSoup(sp_html, \"html.parser\")\n",
        "        # harvest day pages + any direct interviews\n",
        "        for a in sp_soup.select('a[href*=\"show_event.php?event_id=\"]'):\n",
        "            targets.add(norm_url(a.get(\"href\")))\n",
        "        for a in sp_soup.select('a[href*=\"show_interview.php?id=\"]'):\n",
        "            targets.add(norm_url(a.get(\"href\")))\n",
        "\n",
        "    targets = list(targets)\n",
        "    print(f\"{year}: crawling {len(targets)} day/interview pages...\")\n",
        "\n",
        "    # scrape interviews; stream to disk every 20 captures\n",
        "    for targ in tqdm(targets, leave=False, desc=f\"{year} pages\"):\n",
        "        links, (t_title, t_body) = gather_interviews_on_page(session, targ)\n",
        "        # loosen at container level; enforce strictly at interview level below\n",
        "        for iu in links:\n",
        "            if iu in seen_urls:  # skip already saved\n",
        "                continue\n",
        "            ih = safe_get(session, iu)\n",
        "            if not ih:\n",
        "                time.sleep(DELAY);\n",
        "                continue\n",
        "            isoup = BeautifulSoup(ih, \"html.parser\")\n",
        "            i_title = isoup.find(\"title\").get_text(\" \", strip=True) if isoup.find(\"title\") else \"\"\n",
        "            i_body  = isoup.get_text(\" \", strip=True)\n",
        "\n",
        "            # strict postseason check (title/body/url)\n",
        "            if not (is_mlb_postseason(i_title) or is_mlb_postseason(i_body) or is_mlb_postseason(iu)):\n",
        "                continue\n",
        "\n",
        "            transcript = extract_transcript(isoup)\n",
        "            wc = len(transcript.split())\n",
        "            if wc < MIN_WORDS:\n",
        "                continue\n",
        "\n",
        "            row = {\"year\": year, \"interview_url\": iu, \"title\": i_title, \"word_count\": wc, \"transcript\": transcript}\n",
        "            buffer.append(row)\n",
        "            seen_urls.add(iu)\n",
        "            start_added += 1\n",
        "\n",
        "            if len(buffer) >= 20:\n",
        "                flush_rows(buffer)\n",
        "                buffer.clear()\n",
        "                # quick visibility:\n",
        "                print(f\"  ↳ streamed {start_added} for {year} (file size now {OUTPATH.stat().st_size/1024:.1f} KB)\")\n",
        "\n",
        "            time.sleep(DELAY)\n",
        "\n",
        "    # flush any remaining from this year\n",
        "    if buffer:\n",
        "        flush_rows(buffer)\n",
        "        buffer.clear()\n",
        "\n",
        "    per_year_added[year] = start_added\n",
        "    print(f\"✓ {year}: added {start_added} (running total rows: {sum(per_year_added.values())})  →  {OUTPUT}\")\n",
        "\n",
        "print(\"\\nPer-year added:\", per_year_added)\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0ccd09b7317c464f9819379a231042c5",
            "909446d6de4143959ba0f3a8629a046d",
            "478013eee5eb444d9a8dbf3781f2bfc2",
            "e2ec862918e641319115e54e4e3abaff",
            "bbf6cea257834e4b995d50adb6cf5650",
            "8faaca63873846dbb38834cd1c982f39",
            "de7e952c953145618fd08bda5d6696f3",
            "965f18d40f464db181e0d6530a1f36aa",
            "08b2c5748f3f4e5bb747fc85977e1fa6",
            "90282d54205441a1a24ad9814c46966a",
            "d8218db5410844cdb61a0afb31caee40",
            "f250c45fa7704bb0b82371af0438d8e6",
            "d617748476d842739e4c6ff5f455daa7",
            "4bbb94f7d4794f928004090fc08bef9f",
            "0aa0349b252a4e31bb04b90ca3dc29bf",
            "26dc1cbf87814a2da9e241583a30c9cf",
            "0161fa9701264acabb76ccd59fdca487",
            "1407a04855be4851a656cb844388c084",
            "e0c7e7961737425e9490d7fbf599a993",
            "9b41e0a7e7bd4a97ba67c7399ba13fde",
            "45a5e4bae2134d81900b7448f5cf36c7",
            "f6e5921107e94abe8ecc0952e4ceaedf",
            "e5bfdb60ab7647c4a67c1add001a600a",
            "4e4879e2e4214bb7be304f200732e6e0",
            "69156f28c203434787414fb474ac357a",
            "c6af8a5338cb437c90a6ea91d4d2ea0b",
            "3de614af6fc547cfbcac9884072e3743",
            "cfa13bac65394266b10173d5d4ab23d6",
            "9ccf77e0a50e40b6bb4a94c21eeb3a51",
            "a51161a1e59940d3a4b5e255db00d7bc",
            "3827855b3496499aa147afca4d93d289",
            "36762b00a3b3423fa93bac4e21778f81",
            "750a476caf9a4ea29aafa42aad304b72",
            "1ac438b5729644b8b911302cbc61d2bf",
            "130b2dab22ed46e3aeb3ad787bda7aa9",
            "b8e722e7fed24595ae06b29cd1c26cb9",
            "8ed68d5c43124c74a9ad42fdfbb670e6",
            "5ed0056b8262480997cfc73ff27b6b2b",
            "4588dd4bc3ff47668d8033667dbc8ef3",
            "f6b91481fb5a418eabc026220a8c0365",
            "13e2391a53e04992903c2c55f9a0ec88",
            "a262395bc68b46b385dd54c9dadfe4a2",
            "8ca4614632304151b5abe8a7691519eb",
            "715cd36bdd124b638d6dbd933f9abe5a",
            "9ad248dfac104b189dc464a187f25de5",
            "eef6571f908a4405bb6f7aa05544cf1f",
            "eac6003432d04733b3895db61b626a43",
            "ba175477fd4a4949976b5fa0a2d2ff7d",
            "180db26198d34592b5cfb2951abed754",
            "7fd57fe2d8ad45fca0d9d9ce05abb92b",
            "ce664d327c9c4f1f8785ad4d660af579",
            "f147023d62a3483fb16ffdb3b44629a8",
            "19f50f57ca8844e0a6273374d6ed77a7",
            "202207f66fc849d3b0598b956a936199",
            "55b3936d2273466bb29279031bf25636",
            "26c0a5555d6e4b2fb02a34bacd450566",
            "b78f7972a2d2470c8df041d2a261e163",
            "54e4fca5f1bc40b19f6ec790b1ec8f3e",
            "9cb3f4f721b94a5d965c858526ab0445",
            "5a5780044dcb49f19d42b5b9b348f615",
            "f99239f1233c4563a8730194d7700f2c",
            "5b50c05b8972480eb1393831278391b6",
            "b305a94ce66f42b9a27eaab9ad9c48de",
            "fbcdf67e5cdd4aeca8eba9c3ae79a5c9",
            "e375a24b4602475a983642be6cb9943d",
            "8e8a1074f18a4401a2c204506a27a954",
            "da7a9fc13bec420eab493b141fbc3690",
            "dc5b50db7211424c9abec30934dc70ca",
            "94aaf510406846c59d14a30529136ba7",
            "01ef21e80dab49b9b1a920f2aee8850c",
            "0d4e4a3a767f49d1af1c468f4bbd5f6f",
            "4bc8295eac304fdbacff3692be4b6d48",
            "011330e462494648900090eab0d23fe0",
            "c7d5b4f80daa4f47a2aa6a2fb3ed63f1",
            "6701e6a1437a4194911942d550bc8b5d",
            "78e1f6ca7e074083a4cb2632c190758b",
            "8114894cebad4eb1a0b03bdda494c4ca"
          ]
        },
        "id": "vnZnS4HkUXx0",
        "outputId": "3b877016-444c-4c83-ad80-6449347b55d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Years:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ccd09b7317c464f9819379a231042c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2014: 17 series pages, 0 day pages\n",
            "2014: crawling 88 day/interview pages...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2014 pages:   0%|          | 0/88 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f250c45fa7704bb0b82371af0438d8e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ streamed 20 for 2014 (file size now 324.1 KB)\n",
            "  ↳ streamed 40 for 2014 (file size now 562.3 KB)\n",
            "  ↳ streamed 60 for 2014 (file size now 814.7 KB)\n",
            "  ↳ streamed 80 for 2014 (file size now 1131.4 KB)\n",
            "  ↳ streamed 100 for 2014 (file size now 1421.6 KB)\n",
            "  ↳ streamed 120 for 2014 (file size now 1682.7 KB)\n",
            "  ↳ streamed 140 for 2014 (file size now 1957.3 KB)\n",
            "  ↳ streamed 160 for 2014 (file size now 2313.1 KB)\n",
            "  ↳ streamed 180 for 2014 (file size now 2647.5 KB)\n",
            "  ↳ streamed 200 for 2014 (file size now 2932.2 KB)\n",
            "  ↳ streamed 220 for 2014 (file size now 3277.4 KB)\n",
            "  ↳ streamed 240 for 2014 (file size now 3587.7 KB)\n",
            "  ↳ streamed 260 for 2014 (file size now 4002.4 KB)\n",
            "  ↳ streamed 280 for 2014 (file size now 4305.3 KB)\n",
            "✓ 2014: added 290 (running total rows: 290)  →  mlb_postseason_transcripts_2014_2019.csv\n",
            "2015: 19 series pages, 0 day pages\n",
            "2015: crawling 101 day/interview pages...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2015 pages:   0%|          | 0/101 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5bfdb60ab7647c4a67c1add001a600a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ streamed 20 for 2015 (file size now 4812.7 KB)\n",
            "  ↳ streamed 40 for 2015 (file size now 5099.0 KB)\n",
            "  ↳ streamed 60 for 2015 (file size now 5361.6 KB)\n",
            "  ↳ streamed 80 for 2015 (file size now 5633.0 KB)\n",
            "  ↳ streamed 100 for 2015 (file size now 5896.7 KB)\n",
            "  ↳ streamed 120 for 2015 (file size now 6207.9 KB)\n",
            "  ↳ streamed 140 for 2015 (file size now 6549.4 KB)\n",
            "  ↳ streamed 160 for 2015 (file size now 6843.3 KB)\n",
            "  ↳ streamed 180 for 2015 (file size now 7137.0 KB)\n",
            "  ↳ streamed 200 for 2015 (file size now 7399.4 KB)\n",
            "  ↳ streamed 220 for 2015 (file size now 7720.3 KB)\n",
            "  ↳ streamed 240 for 2015 (file size now 8062.2 KB)\n",
            "  ↳ streamed 260 for 2015 (file size now 8307.9 KB)\n",
            "  ↳ streamed 280 for 2015 (file size now 8618.9 KB)\n",
            "  ↳ streamed 300 for 2015 (file size now 8972.2 KB)\n",
            "  ↳ streamed 320 for 2015 (file size now 9221.1 KB)\n",
            "✓ 2015: added 337 (running total rows: 627)  →  mlb_postseason_transcripts_2014_2019.csv\n",
            "2016: 18 series pages, 0 day pages\n",
            "2016: crawling 98 day/interview pages...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2016 pages:   0%|          | 0/98 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ac438b5729644b8b911302cbc61d2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ streamed 20 for 2016 (file size now 9826.6 KB)\n",
            "  ↳ streamed 40 for 2016 (file size now 10138.0 KB)\n",
            "  ↳ streamed 60 for 2016 (file size now 10461.6 KB)\n",
            "  ↳ streamed 80 for 2016 (file size now 10718.0 KB)\n",
            "  ↳ streamed 100 for 2016 (file size now 11039.1 KB)\n",
            "  ↳ streamed 120 for 2016 (file size now 11314.7 KB)\n",
            "  ↳ streamed 140 for 2016 (file size now 11617.0 KB)\n",
            "  ↳ streamed 160 for 2016 (file size now 11956.7 KB)\n",
            "  ↳ streamed 180 for 2016 (file size now 12260.4 KB)\n",
            "  ↳ streamed 200 for 2016 (file size now 12648.3 KB)\n",
            "  ↳ streamed 220 for 2016 (file size now 12988.7 KB)\n",
            "  ↳ streamed 240 for 2016 (file size now 13298.1 KB)\n",
            "  ↳ streamed 260 for 2016 (file size now 13614.6 KB)\n",
            "  ↳ streamed 280 for 2016 (file size now 13930.7 KB)\n",
            "  ↳ streamed 300 for 2016 (file size now 14330.4 KB)\n",
            "✓ 2016: added 314 (running total rows: 941)  →  mlb_postseason_transcripts_2014_2019.csv\n",
            "2017: 28 series pages, 0 day pages\n",
            "2017: crawling 142 day/interview pages...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2017 pages:   0%|          | 0/142 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ad248dfac104b189dc464a187f25de5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ streamed 20 for 2017 (file size now 14830.4 KB)\n",
            "  ↳ streamed 40 for 2017 (file size now 15124.1 KB)\n",
            "  ↳ streamed 60 for 2017 (file size now 15428.0 KB)\n",
            "  ↳ streamed 80 for 2017 (file size now 15721.8 KB)\n",
            "  ↳ streamed 100 for 2017 (file size now 15977.0 KB)\n",
            "  ↳ streamed 120 for 2017 (file size now 16269.6 KB)\n",
            "  ↳ streamed 140 for 2017 (file size now 16576.9 KB)\n",
            "  ↳ streamed 160 for 2017 (file size now 16839.5 KB)\n",
            "  ↳ streamed 180 for 2017 (file size now 17143.8 KB)\n",
            "  ↳ streamed 200 for 2017 (file size now 17458.5 KB)\n",
            "  ↳ streamed 220 for 2017 (file size now 17802.7 KB)\n",
            "  ↳ streamed 240 for 2017 (file size now 18164.2 KB)\n",
            "  ↳ streamed 260 for 2017 (file size now 18476.5 KB)\n",
            "  ↳ streamed 280 for 2017 (file size now 18826.1 KB)\n",
            "  ↳ streamed 300 for 2017 (file size now 19106.8 KB)\n",
            "  ↳ streamed 320 for 2017 (file size now 19431.4 KB)\n",
            "  ↳ streamed 340 for 2017 (file size now 19687.0 KB)\n",
            "✓ 2017: added 347 (running total rows: 1288)  →  mlb_postseason_transcripts_2014_2019.csv\n",
            "2018: 16 series pages, 0 day pages\n",
            "2018: crawling 86 day/interview pages...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2018 pages:   0%|          | 0/86 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26c0a5555d6e4b2fb02a34bacd450566"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ streamed 20 for 2018 (file size now 20127.4 KB)\n",
            "  ↳ streamed 40 for 2018 (file size now 20409.2 KB)\n",
            "  ↳ streamed 60 for 2018 (file size now 20664.1 KB)\n",
            "  ↳ streamed 80 for 2018 (file size now 20947.5 KB)\n",
            "  ↳ streamed 100 for 2018 (file size now 21239.8 KB)\n",
            "  ↳ streamed 120 for 2018 (file size now 21531.0 KB)\n",
            "  ↳ streamed 140 for 2018 (file size now 21853.4 KB)\n",
            "  ↳ streamed 160 for 2018 (file size now 22076.3 KB)\n",
            "  ↳ streamed 180 for 2018 (file size now 22359.3 KB)\n",
            "  ↳ streamed 200 for 2018 (file size now 22664.1 KB)\n",
            "  ↳ streamed 220 for 2018 (file size now 23020.8 KB)\n",
            "  ↳ streamed 240 for 2018 (file size now 23277.0 KB)\n",
            "  ↳ streamed 260 for 2018 (file size now 23596.0 KB)\n",
            "  ↳ streamed 280 for 2018 (file size now 23899.6 KB)\n",
            "  ↳ streamed 300 for 2018 (file size now 24176.7 KB)\n",
            "  ↳ streamed 320 for 2018 (file size now 24472.1 KB)\n",
            "✓ 2018: added 321 (running total rows: 1609)  →  mlb_postseason_transcripts_2014_2019.csv\n",
            "2019: 18 series pages, 0 day pages\n",
            "2019: crawling 96 day/interview pages...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2019 pages:   0%|          | 0/96 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da7a9fc13bec420eab493b141fbc3690"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ↳ streamed 20 for 2019 (file size now 24782.4 KB)\n",
            "  ↳ streamed 40 for 2019 (file size now 25133.3 KB)\n",
            "  ↳ streamed 60 for 2019 (file size now 25434.5 KB)\n",
            "  ↳ streamed 80 for 2019 (file size now 25733.2 KB)\n",
            "  ↳ streamed 100 for 2019 (file size now 26047.5 KB)\n",
            "  ↳ streamed 120 for 2019 (file size now 26363.8 KB)\n",
            "  ↳ streamed 140 for 2019 (file size now 26616.9 KB)\n",
            "  ↳ streamed 160 for 2019 (file size now 26932.6 KB)\n",
            "  ↳ streamed 180 for 2019 (file size now 27236.4 KB)\n",
            "  ↳ streamed 200 for 2019 (file size now 27520.6 KB)\n",
            "  ↳ streamed 220 for 2019 (file size now 27810.4 KB)\n",
            "  ↳ streamed 240 for 2019 (file size now 28119.6 KB)\n",
            "  ↳ streamed 260 for 2019 (file size now 28439.0 KB)\n",
            "  ↳ streamed 280 for 2019 (file size now 28757.7 KB)\n",
            "  ↳ streamed 300 for 2019 (file size now 29009.7 KB)\n",
            "  ↳ streamed 320 for 2019 (file size now 29276.8 KB)\n",
            "  ↳ streamed 340 for 2019 (file size now 29590.3 KB)\n",
            "  ↳ streamed 360 for 2019 (file size now 29820.0 KB)\n",
            "✓ 2019: added 379 (running total rows: 1988)  →  mlb_postseason_transcripts_2014_2019.csv\n",
            "\n",
            "Per-year added: {2014: 290, 2015: 337, 2016: 314, 2017: 347, 2018: 321, 2019: 379}\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVbeF54HdorO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Team Matching\n",
        "\n",
        "import pandas as pd, re, json, time, requests\n",
        "from tqdm.notebook import tqdm\n",
        "from math import ceil\n",
        "\n",
        "enriched = pd.read_csv(ENRICHED_CSV, dtype=str)\n",
        "for c in [\"year\",\"series\",\"matchup\",\"date_iso\",\"interviewee\",\"team_a\",\"team_b\",\"transcript\",\"interview_url\"]:\n",
        "    if c not in enriched.columns: enriched[c] = \"\"\n",
        "\n",
        "# manual overrides\n",
        "manual = {}\n",
        "if os.path.exists(MANUAL_OVERRIDES):\n",
        "    mo = pd.read_csv(MANUAL_OVERRIDES, dtype=str)\n",
        "    for _, r in mo.iterrows():\n",
        "        manual[(str(r[\"interviewee\"]).strip().title(), str(r[\"year\"]).strip())] = str(r[\"team\"]).strip().upper()\n",
        "\n",
        "# cache\n",
        "try:\n",
        "    with open(NAME_CACHE_JSON,\"r\",encoding=\"utf-8\") as f:\n",
        "        name_cache = json.load(f)\n",
        "except:\n",
        "    name_cache = {}\n",
        "\n",
        "# team alias map again (canonical)\n",
        "TEAM_ALIASES = {\n",
        "    \"ARIZONA DIAMONDBACKS\": [\"DIAMONDBACKS\",\"ARIZONA\"],\n",
        "    \"ATLANTA BRAVES\": [\"ATLANTA\",\"BRAVES\"],\n",
        "    \"BALTIMORE ORIOLES\": [\"BALTIMORE\",\"ORIOLES\",\"O'S\",\"OS\"],\n",
        "    \"BOSTON RED SOX\": [\"BOSTON\",\"RED SOX\",\"REDSOX\"],\n",
        "    \"CHICAGO CUBS\": [\"CHICAGO CUBS\",\"CUBS\"],\n",
        "    \"CHICAGO WHITE SOX\": [\"CHICAGO WHITE SOX\",\"WHITE SOX\",\"WHITESOX\",\"SOX\"],\n",
        "    \"CINCINNATI REDS\": [\"CINCINNATI\",\"REDS\"],\n",
        "    \"CLEVELAND GUARDIANS\": [\"CLEVELAND\",\"GUARDIANS\",\"INDIANS\"],\n",
        "    \"COLORADO ROCKIES\": [\"COLORADO\",\"ROCKIES\"],\n",
        "    \"DETROIT TIGERS\": [\"DETROIT\",\"TIGERS\"],\n",
        "    \"HOUSTON ASTROS\": [\"HOUSTON\",\"ASTROS\"],\n",
        "    \"KANSAS CITY ROYALS\": [\"KANSAS CITY\",\"ROYALS\",\"KC\"],\n",
        "    \"LOS ANGELES ANGELS\": [\"LOS ANGELES ANGELS\",\"ANGELS\",\"LAA\",\"ANAHEIM\"],\n",
        "    \"LOS ANGELES DODGERS\": [\"LOS ANGELES DODGERS\",\"DODGERS\",\"LAD\"],\n",
        "    \"MIAMI MARLINS\": [\"MIAMI\",\"MARLINS\",\"FLORIDA MARLINS\"],\n",
        "    \"MILWAUKEE BREWERS\": [\"MILWAUKEE\",\"BREWERS\"],\n",
        "    \"MINNESOTA TWINS\": [\"MINNESOTA\",\"TWINS\"],\n",
        "    \"NEW YORK METS\": [\"NEW YORK METS\",\"METS\",\"NYM\"],\n",
        "    \"NEW YORK YANKEES\": [\"NEW YORK YANKEES\",\"YANKEES\",\"NYY\"],\n",
        "    \"OAKLAND ATHLETICS\": [\"OAKLAND\",\"ATHLETICS\",\"A'S\",\"AS\"],\n",
        "    \"PHILADELPHIA PHILLIES\": [\"PHILADELPHIA\",\"PHILLIES\",\"PHILS\"],\n",
        "    \"PITTSBURGH PIRATES\": [\"PITTSBURGH\",\"PIRATES\",\"BUCS\",\"BUCCOS\"],\n",
        "    \"SAN DIEGO PADRES\": [\"SAN DIEGO\",\"PADRES\"],\n",
        "    \"SAN FRANCISCO GIANTS\": [\"SAN FRANCISCO\",\"GIANTS\"],\n",
        "    \"SEATTLE MARINERS\": [\"SEATTLE\",\"MARINERS\",\"MS\"],\n",
        "    \"ST. LOUIS CARDINALS\": [\"ST. LOUIS\",\"ST LOUIS\",\"CARDINALS\",\"CARDS\"],\n",
        "    \"TAMPA BAY RAYS\": [\"TAMPA BAY\",\"RAYS\",\"DEVIL RAYS\"],\n",
        "    \"TEXAS RANGERS\": [\"TEXAS\",\"RANGERS\"],\n",
        "    \"TORONTO BLUE JAYS\": [\"TORONTO\",\"BLUE JAYS\",\"JAYS\"],\n",
        "    \"WASHINGTON NATIONALS\": [\"WASHINGTON\",\"NATIONALS\",\"NATS\"],\n",
        "}\n",
        "ALIAS_TO_CANON = {alias.upper():team for team,aliases in TEAM_ALIASES.items() for alias in [team]+aliases}\n",
        "\n",
        "def canon_team(t):\n",
        "    return ALIAS_TO_CANON.get(str(t).upper(), str(t).title()) if pd.notna(t) else None\n",
        "\n",
        "# build matchup teams (from parsed columns)\n",
        "def split_matchup(m):\n",
        "    if isinstance(m,str) and \" vs \" in m:\n",
        "        a,b = m.split(\" vs \",1)\n",
        "        return a.strip(), b.strip()\n",
        "    return None, None\n",
        "\n",
        "teamsAB = enriched[\"matchup\"].apply(split_matchup)\n",
        "enriched[\"match_team_a\"] = teamsAB.apply(lambda x: x[0])\n",
        "enriched[\"match_team_b\"] = teamsAB.apply(lambda x: x[1])\n",
        "\n",
        "# --- Wikipedia helper functions (reads Teams row) ---\n",
        "sess = requests.Session()\n",
        "sess.headers.update({\"User-Agent\":\"MLB-TeamResolver/2.1\"})\n",
        "\n",
        "def wiki_search(name):\n",
        "    try:\n",
        "        r = sess.get(\"https://en.wikipedia.org/w/api.php\",\n",
        "                     params={\"action\":\"query\",\"list\":\"search\",\"srsearch\":name,\"format\":\"json\",\"srlimit\":1},\n",
        "                     timeout=15)\n",
        "        return r.json().get(\"query\",{}).get(\"search\",[{}])[0].get(\"title\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def wiki_infobox_html(title):\n",
        "    try:\n",
        "        r = sess.get(\"https://en.wikipedia.org/w/api.php\",\n",
        "                     params={\"action\":\"parse\",\"page\":title,\"prop\":\"text\",\"formatversion\":2,\"format\":\"json\"},\n",
        "                     timeout=20)\n",
        "        return r.json().get(\"parse\",{}).get(\"text\",\"\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_teams(html):\n",
        "    if not html: return []\n",
        "    m = re.search(r\"Teams</th>\\s*<td[^>]*>(.*?)</td>\", html, flags=re.I|re.S)\n",
        "    if not m: return []\n",
        "    text = re.sub(r\"<[^>]+>\",\" \", m.group(1))\n",
        "    text = re.sub(r\"\\s+\",\" \", text).strip()\n",
        "    # split by bullet/semi\n",
        "    chunks = re.split(r\"[;•|]\", text)\n",
        "    return [c.strip() for c in chunks if c.strip()]\n",
        "\n",
        "def pick_from_teams(teams_texts, matchup_set):\n",
        "    # choose the first that mentions one of the matchup teams (by last word or full name)\n",
        "    for t in teams_texts:\n",
        "        for mt in matchup_set:\n",
        "            if not mt: continue\n",
        "            if mt.split()[-1].lower() in t.lower() or mt.lower() in t.lower():\n",
        "                return mt\n",
        "    return None\n",
        "\n",
        "def resolve_team(name, year, teamA, teamB):\n",
        "    nm = str(name).strip()\n",
        "    yr = re.search(r\"(\\d{4})\", str(year))\n",
        "    yr = yr.group(1) if yr else None\n",
        "    if not nm or not yr: return None\n",
        "\n",
        "    # manual override first\n",
        "    key_override = (nm.title(), yr)\n",
        "    if key_override in manual:\n",
        "        return manual[key_override]\n",
        "\n",
        "    cache_key = f\"{nm.title()}|||{yr}\"\n",
        "    if cache_key in name_cache:\n",
        "        return name_cache[cache_key]\n",
        "\n",
        "    title = wiki_search(nm)\n",
        "    if not title:\n",
        "        name_cache[cache_key] = None; return None\n",
        "    html = wiki_infobox_html(title)\n",
        "    teams_texts = extract_teams(html)\n",
        "    choice = pick_from_teams(teams_texts, {teamA, teamB})\n",
        "    name_cache[cache_key] = choice\n",
        "    time.sleep(0.25)\n",
        "    return choice\n",
        "\n",
        "# assign\n",
        "assigned = []\n",
        "for r in tqdm(enriched.itertuples(index=False), total=len(enriched)):\n",
        "    team = resolve_team(getattr(r,\"interviewee\",\"\"), getattr(r,\"year\",\"\"),\n",
        "                        getattr(r,\"match_team_a\",None), getattr(r,\"match_team_b\",None))\n",
        "    assigned.append(team)\n",
        "\n",
        "enriched[\"assigned_team\"] = assigned\n",
        "\n",
        "# persist cache\n",
        "with open(NAME_CACHE_JSON,\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump(name_cache,f,ensure_ascii=False,indent=2)\n",
        "\n",
        "# --- Group and export (only rows with assigned_team) ---\n",
        "to_group = enriched[enriched[\"assigned_team\"].notna()].copy()\n",
        "\n",
        "# Guarantee single 1-D columns & clean strings\n",
        "for col in [\"year\",\"series\",\"matchup\",\"date_iso\",\"assigned_team\",\"interview_url\",\"transcript\"]:\n",
        "    if col not in to_group.columns: to_group[col] = \"\"\n",
        "to_group[\"year\"] = to_group[\"year\"].astype(str).str.extract(r\"(\\d{4})\")[0]\n",
        "\n",
        "group_cols = [\"year\",\"series\",\"matchup\",\"date_iso\",\"assigned_team\"]\n",
        "grouped = (\n",
        "    to_group\n",
        "    .groupby(group_cols, dropna=False, as_index=False)\n",
        "    .agg(\n",
        "        interviews_count=(\"interview_url\",\"count\"),\n",
        "        combined_transcript=(\"transcript\", lambda s: \"\\n\\n\".join(s))\n",
        "    )\n",
        ")\n",
        "grouped[\"combined_word_count\"] = grouped[\"combined_transcript\"].str.split().str.len()\n",
        "\n",
        "# Excel overflow-safe split\n",
        "MAX_CELL = 32767\n",
        "parts_needed = grouped[\"combined_transcript\"].apply(lambda x: ceil(len((x or \"\"))/MAX_CELL))\n",
        "max_parts = int(parts_needed.max()) if len(parts_needed) else 1\n",
        "for j in range(1, max_parts+1):\n",
        "    grouped[f\"transcript_part{j}\"] = \"\"\n",
        "for i, s in grouped[\"combined_transcript\"].items():\n",
        "    if s:\n",
        "        for j in range(0, ceil(len(s)/MAX_CELL)):\n",
        "            grouped.at[i, f\"transcript_part{j+1}\"] = s[j*MAX_CELL:(j+1)*MAX_CELL]\n",
        "\n",
        "grouped.to_excel(GROUPED_XLSX, index=False)\n",
        "grouped.to_parquet(GROUPED_PARQUET, index=False)\n",
        "enriched.to_csv(ENRICHED_CSV, index=False)  # overwrite with assigned_team column\n",
        "\n",
        "print(\"✅ Wrote:\")\n",
        "print(\" -\", ENRICHED_CSV)\n",
        "print(\" -\", GROUPED_XLSX)\n",
        "print(\" -\", GROUPED_PARQUET)\n",
        "\n",
        "# Also save unassigned for quick review\n",
        "enriched[enriched[\"assigned_team\"].isna()][[\"year\",\"series\",\"matchup\",\"date_iso\",\"interviewee\",\"interview_url\"]].to_csv(\n",
        "    \"team_unassigned_rows.csv\", index=False\n",
        ")\n",
        "print(\"Also wrote team_unassigned_rows.csv (for manual review).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "45fb89adac4744bc83559d7ce79733fb",
            "be06dd5fccab47538ae3c0066f771b54",
            "9341875957674ef9848570bdce0e8f1d",
            "95a2088ea26b4e6dae384f5971ff697f",
            "ed64d8086e0140448a562f518b60dd9e",
            "dc669036574e44e0a86a961e8e9fabf9",
            "e5749fcbd437421399a33ed642383414",
            "3d9aff740e184dc2971182fe42c58d55",
            "e6cea98af7fd4396880e888143d99515",
            "492860c3f3cf4499aca9eb80eaec4631",
            "ccef2a112e984082a4e5fc0e65981500"
          ]
        },
        "id": "XDRXVnMZMi9e",
        "outputId": "778b69b7-afdd-42ab-a56f-3f7088c75681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3646 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45fb89adac4744bc83559d7ce79733fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Wrote:\n",
            " - transcripts_enriched.csv\n",
            " - team_game_combined.xlsx\n",
            " - team_game_combined.parquet\n",
            "Also wrote team_unassigned_rows.csv (for manual review).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1InhD0EeFfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rosters, Block 1: Install & Import\n",
        "!pip install -q thefuzz\n",
        "import pandas as pd\n",
        "from thefuzz import process\n",
        "import os\n",
        "\n",
        "print(\"Libraries installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyJh6J4cy3px",
        "outputId": "9135979f-4d09-4f21-ee42-e2ac77d3fbc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rosters, Block 2: Load All Data & Build Smart Lookups (FINAL FIX)\n",
        "\n",
        "# --- Define file paths (they are now in the main directory) ---\n",
        "PEOPLE_CSV = 'People.csv'\n",
        "BATTING_CSV = 'Batting.csv'\n",
        "PITCHING_CSV = 'Pitching.csv'\n",
        "MANAGERS_CSV = 'Managers.csv'\n",
        "\n",
        "# --- Confirm your file names ---\n",
        "YOUR_FILE_NAME = 'transcripts_enriched.csv'  # <-- I've kept this from your last message\n",
        "PLAYER_NAME_COLUMN = 'interviewee'\n",
        "YEAR_COLUMN = 'year'\n",
        "# -----------------------------\n",
        "\n",
        "try:\n",
        "    # --- Load Your Data ---\n",
        "    user_df = pd.read_csv(YOUR_FILE_NAME)\n",
        "    print(f\"Successfully loaded '{YOUR_FILE_NAME}'. Original shape: {user_df.shape}\")\n",
        "\n",
        "    # 1. Clean the interviewee name\n",
        "    user_df[PLAYER_NAME_COLUMN] = user_df[PLAYER_NAME_COLUMN].str.strip()\n",
        "\n",
        "    # 2. Deduplicate\n",
        "    dedupe_cols = ['interviewee', 'date_iso', 'word_count']\n",
        "    rows_before = len(user_df)\n",
        "    user_df = user_df.drop_duplicates(subset=dedupe_cols, keep='first')\n",
        "    rows_after = len(user_df)\n",
        "    print(f\"Deduplicated: Removed {rows_before - rows_after} rows. New shape: {user_df.shape}\")\n",
        "\n",
        "    # --- Build Lahman Lookups from Uploaded CSVs ---\n",
        "    print(\"\\nBuilding smart name lookup...\")\n",
        "    people_df = pd.read_csv(PEOPLE_CSV)\n",
        "\n",
        "    all_names_df = people_df[['playerID', 'nameFirst', 'nameLast', 'debut']].copy()\n",
        "    all_names_df['Name'] = all_names_df['nameFirst'] + ' ' + all_names_df['nameLast']\n",
        "    all_names_df = all_names_df.sort_values('debut', ascending=False)\n",
        "\n",
        "    people_lookup = all_names_df.drop_duplicates('Name').set_index('Name')['playerID']\n",
        "    all_lahman_names = people_lookup.index.tolist()\n",
        "    print(f\"Created 'people' lookup with {len(all_lahman_names)} unique names.\")\n",
        "\n",
        "    # --- Build UP-TO-DATE 'team' lookup (THE FIX IS HERE) ---\n",
        "    print(\"Building up-to-date 'team' lookup (prioritizing *last* team)...\")\n",
        "    batting_df = pd.read_csv(BATTING_CSV)\n",
        "    pitching_df = pd.read_csv(PITCHING_CSV)\n",
        "    managers_df = pd.read_csv(MANAGERS_CSV)\n",
        "\n",
        "    # Get player stints (using 'stint' column)\n",
        "    player_stints = pd.concat([\n",
        "        batting_df[['playerID', 'yearID', 'teamID', 'stint']],\n",
        "        pitching_df[['playerID', 'yearID', 'teamID', 'stint']]\n",
        "    ])\n",
        "\n",
        "    # Get manager stints (using 'inseason' as the 'stint')\n",
        "    manager_stints = managers_df[['playerID', 'yearID', 'teamID', 'inseason']].rename(\n",
        "        columns={'inseason': 'stint'}\n",
        "    )\n",
        "\n",
        "    # Combine all stints\n",
        "    all_stints = pd.concat([player_stints, manager_stints])\n",
        "\n",
        "    # --- THIS IS THE KEY CHANGE ---\n",
        "    # We sort by 'stint' (descending) to get the highest stint number (the last team)\n",
        "    # Then we drop duplicates, keeping the first one (which is now the last team)\n",
        "    final_team = all_stints.sort_values('stint', ascending=False).drop_duplicates(['playerID', 'yearID'])\n",
        "\n",
        "    team_lookup = final_team.set_index(['playerID', 'yearID'])['teamID']\n",
        "    print(\"Master lookups are built and ready.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"--- !!! FILE NOT FOUND ERROR !!! ---\")\n",
        "    print(f\"Could not find file: {e.filename}\")\n",
        "    print(\"Please make sure you have uploaded all 5 files to Colab.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzpCc2RMzOcD",
        "outputId": "64b44306-0ab4-4f14-dbb6-7d3d34715c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 'transcripts_enriched.csv'. Original shape: (3646, 14)\n",
            "Deduplicated: Removed 10 rows. New shape: (3636, 14)\n",
            "\n",
            "Building smart name lookup...\n",
            "Created 'people' lookup with 20566 unique names.\n",
            "Building up-to-date 'team' lookup (prioritizing *last* team)...\n",
            "Master lookups are built and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rosters Block 3: Define the Fuzzy Matching Function\n",
        "\n",
        "def find_team_fuzzy(row):\n",
        "    # Get the name and year from the row\n",
        "    name_to_find = row[PLAYER_NAME_COLUMN]\n",
        "    year_to_find = row[YEAR_COLUMN]\n",
        "\n",
        "    # 1. Check for a perfect match first (fast)\n",
        "    player_id = people_lookup.get(name_to_find)\n",
        "\n",
        "    # 2. If no perfect match, use fuzzy matching\n",
        "    if player_id is None:\n",
        "        try:\n",
        "            best_match = process.extractOne(name_to_find, all_lahman_names, score_cutoff=85)\n",
        "\n",
        "            if best_match:\n",
        "                matched_name = best_match[0]\n",
        "                player_id = people_lookup.get(matched_name)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    # 3. If we have a player_id, find the team\n",
        "    if player_id:\n",
        "        team = team_lookup.get((player_id, year_to_find))\n",
        "        return team\n",
        "\n",
        "    # 4. If all else fails, return None\n",
        "    return None\n",
        "\n",
        "print(\"Matcher function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbrf77ayzWb0",
        "outputId": "b2a9c610-b3cd-4b3f-b852-07c9ef2cd5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matcher function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rosters Block 4: Apply Function, Review, and Save\n",
        "\n",
        "print(\"Applying fuzzy match to all rows... This may take a minute.\")\n",
        "\n",
        "# .apply() runs the function on every single row\n",
        "user_df['Team'] = user_df.apply(find_team_fuzzy, axis=1)\n",
        "\n",
        "print(\"Matching complete.\")\n",
        "\n",
        "# --- Review ---\n",
        "print(\"\\n--- Review Final Data ---\")\n",
        "print(user_df.head(20))\n",
        "\n",
        "# Show a summary\n",
        "total_rows = len(user_df)\n",
        "matched_rows = user_df['Team'].notna().sum()\n",
        "unmatched_rows = user_df['Team'].isna().sum()\n",
        "\n",
        "print(f\"\\n--- FINAL Match Summary (Manual Upload) ---\")\n",
        "print(f\"Total Rows: {total_rows}\")\n",
        "print(f\"Rows with Team Found: {matched_rows}\")\n",
        "print(f\"Rows with No Team Found: {unmatched_rows}\")\n",
        "\n",
        "if unmatched_rows > 0:\n",
        "    print(\"\\nNOTE: Remaining unmatched rows are likely non-players (coaches, execs),\")\n",
        "    print(\"names with very bad misspellings, or players inactive that year.\")\n",
        "    print(\"\\nSee 'unmatched_rows.csv' for a list of these.\")\n",
        "\n",
        "    unmatched_df = user_df[user_df['Team'].isna()]\n",
        "    unmatched_df.to_csv('unmatched_rows.csv', index=False)\n",
        "\n",
        "\n",
        "# Save the final, merged data to a new CSV\n",
        "OUTPUT_FILE = 'your_data_with_teams_FINAL.csv'\n",
        "user_df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"\\nSuccessfully saved your new file as: {OUTPUT_FILE}\")\n",
        "print(\"You can find it in the Colab sidebar (folder icon) and download it.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXw7BmmgzZDQ",
        "outputId": "f6b65dc8-ad96-4710-8a2f-3c54b12e8498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying fuzzy match to all rows... This may take a minute.\n",
            "Matching complete.\n",
            "\n",
            "--- Review Final Data ---\n",
            "                                        interview_url  \\\n",
            "0   https://www.asapsports.com/show_interview.php?...   \n",
            "1   https://www.asapsports.com/show_interview.php?...   \n",
            "2   https://www.asapsports.com/show_interview.php?...   \n",
            "3   https://www.asapsports.com/show_interview.php?...   \n",
            "4   https://www.asapsports.com/show_interview.php?...   \n",
            "5   https://www.asapsports.com/show_interview.php?...   \n",
            "6   https://www.asapsports.com/show_interview.php?...   \n",
            "7   https://www.asapsports.com/show_interview.php?...   \n",
            "8   https://www.asapsports.com/show_interview.php?...   \n",
            "9   https://www.asapsports.com/show_interview.php?...   \n",
            "10  https://www.asapsports.com/show_interview.php?...   \n",
            "11  https://www.asapsports.com/show_interview.php?...   \n",
            "12  https://www.asapsports.com/show_interview.php?...   \n",
            "13  https://www.asapsports.com/show_interview.php?...   \n",
            "14  https://www.asapsports.com/show_interview.php?...   \n",
            "15  https://www.asapsports.com/show_interview.php?...   \n",
            "16  https://www.asapsports.com/show_interview.php?...   \n",
            "17  https://www.asapsports.com/show_interview.php?...   \n",
            "18  https://www.asapsports.com/show_interview.php?...   \n",
            "19  https://www.asapsports.com/show_interview.php?...   \n",
            "\n",
            "                                                title  word_count  \\\n",
            "0   ASAP Sports Transcripts - Baseball - 2014 - NL...        3711   \n",
            "1   ASAP Sports Transcripts - Baseball - 2014 - NL...        3749   \n",
            "2   ASAP Sports Transcripts - Baseball - 2014 - NL...        3159   \n",
            "3   ASAP Sports Transcripts - Baseball - 2014 - NL...        1447   \n",
            "4   ASAP Sports Transcripts - Baseball - 2014 - NL...        2983   \n",
            "5   ASAP Sports Transcripts - Baseball - 2014 - NL...        1157   \n",
            "6   ASAP Sports Transcripts - Baseball - 2014 - NL...        3610   \n",
            "7   ASAP Sports Transcripts - Baseball - 2014 - AL...        1973   \n",
            "8   ASAP Sports Transcripts - Baseball - 2014 - AL...        2989   \n",
            "9   ASAP Sports Transcripts - Baseball - 2014 - AL...        2462   \n",
            "10  ASAP Sports Transcripts - Baseball - 2014 - AL...        1037   \n",
            "11  ASAP Sports Transcripts - Baseball - 2014 - AL...        2115   \n",
            "12  ASAP Sports Transcripts - Baseball - 2014 - AL...        1941   \n",
            "13  ASAP Sports Transcripts - Baseball - 2014 - AL...        2093   \n",
            "14  ASAP Sports Transcripts - Baseball - 2014 - NL...        4788   \n",
            "15  ASAP Sports Transcripts - Baseball - 2014 - NL...        5628   \n",
            "16  ASAP Sports Transcripts - Baseball - 2014 - NL...        2234   \n",
            "17  ASAP Sports Transcripts - Baseball - 2014 - NL...        4778   \n",
            "18  ASAP Sports Transcripts - Baseball - 2014 - AL...        4903   \n",
            "19  ASAP Sports Transcripts - Baseball - 2014 - AL...        4483   \n",
            "\n",
            "                                           transcript  year  \\\n",
            "0   Q.Â Would Lackey or Lynn be available tonight ...  2014   \n",
            "1   Q.Â To have originally come up in this organiz...  2014   \n",
            "2   Q.Â I know you don't have to win everything to...  2014   \n",
            "3   Q.Â Tell me about the competition tonight, you...  2014   \n",
            "4   Q.Â Is there anyone who is not available for t...  2014   \n",
            "5   Q.Â The continuity in the last five years with...  2014   \n",
            "6   Q.Â Your manager always seems to think you hav...  2014   \n",
            "7   Q.Â Brad Ausmus said, in his words, your last ...  2014   \n",
            "8   Q.Â Buck, could you explain some of your roste...  2014   \n",
            "9   Q.Â Assuming you're anticipating close games i...  2014   \n",
            "10  Q.Â Nelson, obviously it was such a close game...  2014   \n",
            "11  Q.Â How important is having postseason experie...  2014   \n",
            "12  Q.Â Rajai Davis, what he's been able to do and...  2014   \n",
            "13  Q.Â Brad, when Torii lined into the double pla...  2014   \n",
            "14  Q.Â Adam, how is the right elbow? ADAM WAINWRI...  2014   \n",
            "15  Q.Â Mike, do you have any changes in your rost...  2014   \n",
            "16  Q.Â Madison, when Wainwright was in here, I as...  2014   \n",
            "17  Q.Â Bruce, do you have your roster set yet?Â A...  2014   \n",
            "18  Q.Â What led you to go with Vargas as your sta...  2014   \n",
            "19  Q.Â How valuable is your experience in postsea...  2014   \n",
            "\n",
            "                    series                                      matchup  \\\n",
            "0   NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "1   NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "2   NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "3   NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "4   NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "5   NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "6   NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "7       AL DIVISION SERIES          BALTIMORE ORIOLES vs DETROIT TIGERS   \n",
            "8       AL DIVISION SERIES          BALTIMORE ORIOLES vs DETROIT TIGERS   \n",
            "9       AL DIVISION SERIES          BALTIMORE ORIOLES vs DETROIT TIGERS   \n",
            "10      AL DIVISION SERIES          BALTIMORE ORIOLES vs DETROIT TIGERS   \n",
            "11      AL DIVISION SERIES          BALTIMORE ORIOLES vs DETROIT TIGERS   \n",
            "12      AL DIVISION SERIES          BALTIMORE ORIOLES vs DETROIT TIGERS   \n",
            "13      AL DIVISION SERIES          BALTIMORE ORIOLES vs DETROIT TIGERS   \n",
            "14  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "15  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "16  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "17  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "18      AL DIVISION SERIES     KANSAS CITY ROYALS vs LOS ANGELES ANGELS   \n",
            "19      AL DIVISION SERIES     KANSAS CITY ROYALS vs LOS ANGELES ANGELS   \n",
            "\n",
            "      date_iso        interviewee                team_a               team_b  \\\n",
            "0   2014-10-16       Mike Matheny  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "1   2014-10-16      Michael Morse  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "2   2014-10-16     Matt Carpenter  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "3   2014-10-16  Madison Bumgarner  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "4   2014-10-16        Bruce Bochy  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "5   2014-10-16        Bruce Bochy  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "6   2014-10-16     Jeremy Affeldt  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "7   2014-10-02   Justin Verlander        DETROIT TIGERS    BALTIMORE ORIOLES   \n",
            "8   2014-10-02     Buck Showalter        DETROIT TIGERS    BALTIMORE ORIOLES   \n",
            "9   2014-10-02     Buck Showalter        DETROIT TIGERS    BALTIMORE ORIOLES   \n",
            "10  2014-10-02        Nelson Cruz        DETROIT TIGERS    BALTIMORE ORIOLES   \n",
            "11  2014-10-02       Wei-Yin Chen        DETROIT TIGERS    BALTIMORE ORIOLES   \n",
            "12  2014-10-02        Brad Ausmus        DETROIT TIGERS    BALTIMORE ORIOLES   \n",
            "13  2014-10-02        Brad Ausmus        DETROIT TIGERS    BALTIMORE ORIOLES   \n",
            "14  2014-10-10    Adam Wainwright  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "15  2014-10-10       Mike Matheny  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "16  2014-10-10  Madison Bumgarner  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "17  2014-10-10        Bruce Bochy  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "18  2014-10-01           Ned Yost    KANSAS CITY ROYALS   LOS ANGELES ANGELS   \n",
            "19  2014-10-01       Jered Weaver    KANSAS CITY ROYALS   LOS ANGELES ANGELS   \n",
            "\n",
            "            match_team_a         match_team_b assigned_team  Team  \n",
            "0   SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SLN  \n",
            "1   SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN  None  \n",
            "2   SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SLN  \n",
            "3   SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SFN  \n",
            "4   SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SFN  \n",
            "5   SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SFN  \n",
            "6   SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SFN  \n",
            "7      BALTIMORE ORIOLES       DETROIT TIGERS           NaN   DET  \n",
            "8      BALTIMORE ORIOLES       DETROIT TIGERS           NaN   BAL  \n",
            "9      BALTIMORE ORIOLES       DETROIT TIGERS           NaN   BAL  \n",
            "10     BALTIMORE ORIOLES       DETROIT TIGERS           NaN   BAL  \n",
            "11     BALTIMORE ORIOLES       DETROIT TIGERS           NaN   BAL  \n",
            "12     BALTIMORE ORIOLES       DETROIT TIGERS           NaN   DET  \n",
            "13     BALTIMORE ORIOLES       DETROIT TIGERS           NaN   DET  \n",
            "14  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SLN  \n",
            "15  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SLN  \n",
            "16  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SFN  \n",
            "17  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS           NaN   SFN  \n",
            "18    KANSAS CITY ROYALS   LOS ANGELES ANGELS           NaN   KCA  \n",
            "19    KANSAS CITY ROYALS   LOS ANGELES ANGELS           NaN   LAA  \n",
            "\n",
            "--- FINAL Match Summary (Manual Upload) ---\n",
            "Total Rows: 3636\n",
            "Rows with Team Found: 3502\n",
            "Rows with No Team Found: 134\n",
            "\n",
            "NOTE: Remaining unmatched rows are likely non-players (coaches, execs),\n",
            "names with very bad misspellings, or players inactive that year.\n",
            "\n",
            "See 'unmatched_rows.csv' for a list of these.\n",
            "\n",
            "Successfully saved your new file as: your_data_with_teams_FINAL.csv\n",
            "You can find it in the Colab sidebar (folder icon) and download it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rosters Block 5: Drop Unnecessary Columns (Revised)\n",
        "\n",
        "# This will DROP 'team_a', 'team_b', and 'assigned_team'\n",
        "# It will keep the 'match_team' columns.\n",
        "columns_to_drop = ['team_a', 'team_b', 'assigned_team']\n",
        "\n",
        "# To keep 'team_a' and 'team_b' (and drop the 'match_team' columns instead),\n",
        "# uncomment the line below:\n",
        "# columns_to_drop = ['match_team_a', 'match_team_b', 'assigned_team']\n",
        "\n",
        "# --- RUN THE CODE ---\n",
        "try:\n",
        "    # Check if the dataframe exists\n",
        "    if 'user_df' not in locals():\n",
        "        print(\"Error: 'user_df' not found.\")\n",
        "        print(\"Please re-run Block 2 to load your data, then run this block again.\")\n",
        "\n",
        "    # Check if the columns exist before trying to drop\n",
        "    elif all(col in user_df.columns for col in columns_to_drop):\n",
        "        user_df = user_df.drop(columns=columns_to_drop)\n",
        "        print(f\"Successfully dropped the columns: {columns_to_drop}\")\n",
        "        print(\"Here is a preview of the new data structure:\")\n",
        "        print(user_df.head())\n",
        "\n",
        "        # Save the file again with the new, cleaner data\n",
        "        OUTPUT_FILE = 'your_data_FINAL_cleaned.csv'\n",
        "        user_df.to_csv(OUTPUT_FILE, index=False)\n",
        "        print(f\"\\nSuccessfully saved the cleaned data to: {OUTPUT_FILE}\")\n",
        "        print(\"You can now download this new file.\")\n",
        "    else:\n",
        "        # Check which columns are missing\n",
        "        missing_cols = [col for col in columns_to_drop if col not in user_df.columns]\n",
        "        print(f\"Error: The following columns were not found: {missing_cols}\")\n",
        "        print(\"They might have been dropped already, or the names are incorrect.\")\n",
        "        print(\"Current columns:\", user_df.columns.tolist())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvxRAlrg8Ztr",
        "outputId": "b5a067ff-c9e6-4d0a-83b5-040a69624ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The following columns were not found: ['team_a', 'team_b']\n",
            "They might have been dropped already, or the names are incorrect.\n",
            "Current columns: ['interview_url', 'title', 'word_count', 'transcript', 'year', 'series', 'matchup', 'date_iso', 'interviewee', 'match_team_a', 'match_team_b', 'assigned_team', 'Team']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rosters Block 6: Drop the last remaining column\n",
        "\n",
        "# 'team_a' and 'team_b' are already gone.\n",
        "# We only need to drop 'assigned_team'.\n",
        "columns_to_drop = ['assigned_team']\n",
        "\n",
        "# --- RUN THE CODE ---\n",
        "try:\n",
        "    # Check if the dataframe exists\n",
        "    if 'user_df' not in locals():\n",
        "        print(\"Error: 'user_df' not found.\")\n",
        "        print(\"Please re-run Block 2 to load your data, then run this block again.\")\n",
        "\n",
        "    # Check if the columns exist before trying to drop\n",
        "    elif all(col in user_df.columns for col in columns_to_drop):\n",
        "        user_df = user_df.drop(columns=columns_to_drop)\n",
        "        print(f\"Successfully dropped the columns: {columns_to_drop}\")\n",
        "        print(\"Here is a preview of the new data structure:\")\n",
        "        print(user_df.head())\n",
        "\n",
        "        # Save the file again with the new, cleaner data\n",
        "        OUTPUT_FILE = 'your_data_FINAL_cleaned.csv'\n",
        "        user_df.to_csv(OUTPUT_FILE, index=False)\n",
        "        print(f\"\\nSuccessfully saved the cleaned data to: {OUTPUT_FILE}\")\n",
        "        print(\"You can now download this new file.\")\n",
        "    else:\n",
        "        # Check which columns are missing\n",
        "        missing_cols = [col for col in columns_to_drop if col not in user_df.columns]\n",
        "        print(f\"Error: The following columns were not found: {missing_cols}\")\n",
        "        print(\"They might have been dropped already, or the names are incorrect.\")\n",
        "        print(\"Current columns:\", user_df.columns.tolist())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "I8NEYy6B8mS6",
        "outputId": "38695e22-ce9e-4153-ff59-b5e8f2efda1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully dropped the columns: ['assigned_team']\n",
            "Here is a preview of the new data structure:\n",
            "                                       interview_url  \\\n",
            "0  https://www.asapsports.com/show_interview.php?...   \n",
            "1  https://www.asapsports.com/show_interview.php?...   \n",
            "2  https://www.asapsports.com/show_interview.php?...   \n",
            "3  https://www.asapsports.com/show_interview.php?...   \n",
            "4  https://www.asapsports.com/show_interview.php?...   \n",
            "\n",
            "                                               title  word_count  \\\n",
            "0  ASAP Sports Transcripts - Baseball - 2014 - NL...        3711   \n",
            "1  ASAP Sports Transcripts - Baseball - 2014 - NL...        3749   \n",
            "2  ASAP Sports Transcripts - Baseball - 2014 - NL...        3159   \n",
            "3  ASAP Sports Transcripts - Baseball - 2014 - NL...        1447   \n",
            "4  ASAP Sports Transcripts - Baseball - 2014 - NL...        2983   \n",
            "\n",
            "                                          transcript  year  \\\n",
            "0  Q.Â Would Lackey or Lynn be available tonight ...  2014   \n",
            "1  Q.Â To have originally come up in this organiz...  2014   \n",
            "2  Q.Â I know you don't have to win everything to...  2014   \n",
            "3  Q.Â Tell me about the competition tonight, you...  2014   \n",
            "4  Q.Â Is there anyone who is not available for t...  2014   \n",
            "\n",
            "                   series                                      matchup  \\\n",
            "0  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "1  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "2  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "3  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "4  NL CHAMPIONSHIP SERIES  SAN FRANCISCO GIANTS vs ST. LOUIS CARDINALS   \n",
            "\n",
            "     date_iso        interviewee          match_team_a         match_team_b  \\\n",
            "0  2014-10-16       Mike Matheny  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "1  2014-10-16      Michael Morse  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "2  2014-10-16     Matt Carpenter  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "3  2014-10-16  Madison Bumgarner  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "4  2014-10-16        Bruce Bochy  SAN FRANCISCO GIANTS  ST. LOUIS CARDINALS   \n",
            "\n",
            "   Team  \n",
            "0   SLN  \n",
            "1  None  \n",
            "2   SLN  \n",
            "3   SFN  \n",
            "4   SFN  \n",
            "\n",
            "Successfully saved the cleaned data to: your_data_FINAL_cleaned.csv\n",
            "You can now download this new file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Playoff Roster Creation\n",
        "# %%capture\n",
        "!pip -q install pandas requests tqdm\n",
        "\n",
        "# %%\n",
        "import time, json, requests, pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -----------------------------------\n",
        "# Config\n",
        "# -----------------------------------\n",
        "YEARS = [y for y in range(2014, 2025) if y != 2020]  # 2014–2024, skip 2020\n",
        "STATS_BASE = \"https://statsapi.mlb.com/api/v1\"\n",
        "HEADERS = {\"User-Agent\":\"Mozilla/5.0 (compatible; postseason-rosters/2.0)\"}\n",
        "\n",
        "# Map MLB StatsAPI teamId -> Retrosheet-style 3-letter codes (to join your CSV)\n",
        "TEAMID_TO_RETRO = {\n",
        "    110:\"BAL\",111:\"BOS\",147:\"NYA\",139:\"TBA\",141:\"TOR\",\n",
        "    145:\"CHA\",114:\"CLE\",116:\"DET\",118:\"KCA\",142:\"MIN\",\n",
        "    117:\"HOU\",108:\"LAA\",133:\"OAK\",136:\"SEA\",140:\"TEX\",\n",
        "    144:\"ATL\",146:\"MIA\",121:\"NYN\",143:\"PHI\",120:\"WAS\",\n",
        "    112:\"CHN\",113:\"CIN\",158:\"MIL\",134:\"PIT\",138:\"SLN\",\n",
        "    109:\"ARI\",115:\"COL\",119:\"LAN\",135:\"SDN\",137:\"SFN\",\n",
        "}\n",
        "\n",
        "def series_tag_from_desc(desc: str) -> str:\n",
        "    if not isinstance(desc, str): return \"\"\n",
        "    d = desc.lower()\n",
        "    if \"wild\" in d: return \"wc\"\n",
        "    if \"division\" in d: return \"ds\"\n",
        "    if \"championship\" in d: return \"lcs\"\n",
        "    if \"world series\" in d or \"ws\" in d: return \"ws\"\n",
        "    return \"\"\n",
        "\n",
        "# -----------------------------------\n",
        "# Tiny HTTP client with caching/retries\n",
        "# -----------------------------------\n",
        "class Http:\n",
        "    def __init__(self):\n",
        "        self.sess = requests.Session()\n",
        "        self.cache = {}\n",
        "    def get_json(self, url, params=None, timeout=25, retries=3, backoff=0.6):\n",
        "        key = (url, tuple(sorted((params or {}).items())))\n",
        "        if key in self.cache:\n",
        "            return self.cache[key]\n",
        "        last = None\n",
        "        for i in range(retries):\n",
        "            try:\n",
        "                r = self.sess.get(url, params=params, headers=HEADERS, timeout=timeout)\n",
        "                r.raise_for_status()\n",
        "                js = r.json()\n",
        "                self.cache[key] = js\n",
        "                return js\n",
        "            except Exception as e:\n",
        "                last = e\n",
        "                time.sleep(backoff * (i+1))\n",
        "        raise last\n",
        "\n",
        "http = Http()\n",
        "\n",
        "# -----------------------------------\n",
        "# 1) Pull all postseason games for each year\n",
        "# Prefer /schedule/postseason, fall back to /schedule with gameTypes F,D,L,W\n",
        "# -----------------------------------\n",
        "def get_postseason_games_year(year: int):\n",
        "    games = []\n",
        "    # Preferred endpoint\n",
        "    try:\n",
        "        js = http.get_json(f\"{STATS_BASE}/schedule/postseason\", params={\"season\": year, \"sportId\": 1})\n",
        "        for d in js.get(\"dates\", []):\n",
        "            for g in d.get(\"games\", []):\n",
        "                if not g.get(\"gamePk\"): continue\n",
        "                games.append({\n",
        "                    \"year\": year,\n",
        "                    \"date\": g.get(\"officialDate\") or (g.get(\"gameDate\",\"\")[:10]),\n",
        "                    \"gamePk\": int(g[\"gamePk\"]),\n",
        "                    \"homeId\": g.get(\"teams\",{}).get(\"home\",{}).get(\"team\",{}).get(\"id\"),\n",
        "                    \"awayId\": g.get(\"teams\",{}).get(\"away\",{}).get(\"team\",{}).get(\"id\"),\n",
        "                    \"series_desc\": g.get(\"seriesDescription\") or g.get(\"seriesDescriptionShort\") or \"\",\n",
        "                })\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if games:\n",
        "        return games\n",
        "\n",
        "    # Fallback endpoint with explicit gameTypes\n",
        "    try:\n",
        "        js = http.get_json(f\"{STATS_BASE}/schedule\", params={\"sportId\":1, \"season\":year, \"gameTypes\":\"F,D,L,W\"})\n",
        "        for d in js.get(\"dates\", []):\n",
        "            for g in d.get(\"games\", []):\n",
        "                if not g.get(\"gamePk\"): continue\n",
        "                games.append({\n",
        "                    \"year\": year,\n",
        "                    \"date\": g.get(\"officialDate\") or (g.get(\"gameDate\",\"\")[:10]),\n",
        "                    \"gamePk\": int(g[\"gamePk\"]),\n",
        "                    \"homeId\": g.get(\"teams\",{}).get(\"home\",{}).get(\"team\",{}).get(\"id\"),\n",
        "                    \"awayId\": g.get(\"teams\",{}).get(\"away\",{}).get(\"team\",{}).get(\"id\"),\n",
        "                    \"series_desc\": g.get(\"seriesDescription\") or g.get(\"seriesDescriptionShort\") or \"\",\n",
        "                })\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return games\n",
        "\n",
        "all_games = []\n",
        "for y in YEARS:\n",
        "    all_games.extend(get_postseason_games_year(y))\n",
        "\n",
        "games_df = pd.DataFrame(all_games).drop_duplicates()\n",
        "if games_df.empty:\n",
        "    raise RuntimeError(\"No postseason games found from the API endpoints.\")\n",
        "\n",
        "# Keep only games where both team IDs map cleanly\n",
        "mask = games_df[\"homeId\"].isin(TEAMID_TO_RETRO) & games_df[\"awayId\"].isin(TEAMID_TO_RETRO)\n",
        "games_df = games_df[mask].reset_index(drop=True)\n",
        "\n",
        "# -----------------------------------\n",
        "# 2) Boxscores -> full game roster (includes DNPs)\n",
        "# Correct JSON path: js['teams']['home'|'away']['players']\n",
        "# -----------------------------------\n",
        "def fetch_boxscore_roster(game_pk: int):\n",
        "    js = http.get_json(f\"{STATS_BASE}/game/{game_pk}/boxscore\")\n",
        "    teams = js.get(\"teams\", {}) or {}\n",
        "    out = []\n",
        "    for side in (\"home\",\"away\"):\n",
        "        tnode = teams.get(side, {}) or {}\n",
        "        tid = (tnode.get(\"team\") or {}).get(\"id\")\n",
        "        players = (tnode.get(\"players\") or {})\n",
        "        for pdata in players.values():\n",
        "            person = pdata.get(\"person\", {}) or {}\n",
        "            pos = pdata.get(\"position\", {}) or {}\n",
        "            out.append({\n",
        "                \"teamId\": tid,\n",
        "                \"mlb_player_id\": person.get(\"id\"),\n",
        "                \"fullName\": person.get(\"fullName\"),\n",
        "                \"primaryNumber\": pdata.get(\"jerseyNumber\"),\n",
        "                \"position_code\": pos.get(\"code\"),\n",
        "                \"position_name\": pos.get(\"name\"),\n",
        "            })\n",
        "    return out\n",
        "\n",
        "# -----------------------------------\n",
        "# 3) Build YEAR × TEAM union rosters (no daily/series outputs)\n",
        "# -----------------------------------\n",
        "# Use dict: {(year, teamId) -> {player_id -> info}}\n",
        "year_team_players = {}\n",
        "\n",
        "print(\"Fetching boxscores & unioning to year-level rosters…\")\n",
        "for _, g in tqdm(games_df.iterrows(), total=len(games_df)):\n",
        "    pk = int(g[\"gamePk\"])\n",
        "    y = int(g[\"year\"])\n",
        "    try:\n",
        "        roster = fetch_boxscore_roster(pk)\n",
        "    except Exception as e:\n",
        "        # log and continue\n",
        "        continue\n",
        "\n",
        "    for tid in (g[\"homeId\"], g[\"awayId\"]):\n",
        "        bucket = year_team_players.setdefault((y, int(tid)), {})\n",
        "        for r in roster:\n",
        "            if r.get(\"teamId\") != tid:\n",
        "                continue\n",
        "            pid = r.get(\"mlb_player_id\")\n",
        "            if not pid:\n",
        "                continue\n",
        "            # keep first seen info (names/positions)\n",
        "            if pid not in bucket:\n",
        "                bucket[pid] = {\n",
        "                    \"mlb_player_id\": int(pid),\n",
        "                    \"fullName\": r.get(\"fullName\"),\n",
        "                    \"primaryNumber\": r.get(\"primaryNumber\"),\n",
        "                    \"position_code\": r.get(\"position_code\"),\n",
        "                    \"position_name\": r.get(\"position_name\"),\n",
        "                }\n",
        "    time.sleep(0.02)\n",
        "\n",
        "# Materialize rows\n",
        "rows = []\n",
        "for (year, teamId), plist in year_team_players.items():\n",
        "    team_code = TEAMID_TO_RETRO.get(int(teamId))\n",
        "    for info in plist.values():\n",
        "        rows.append({\n",
        "            \"year\": year,\n",
        "            \"teamId\": int(teamId),\n",
        "            \"team\": team_code,\n",
        "            **info\n",
        "        })\n",
        "\n",
        "out = pd.DataFrame(rows).drop_duplicates().sort_values([\"year\",\"team\",\"fullName\"]).reset_index(drop=True)\n",
        "\n",
        "# Save: one roster per team per year (union of all postseason games that year)\n",
        "out_path = \"/content/mlb_postseason_year_team_rosters_2014_2024.csv\"\n",
        "out.to_csv(out_path, index=False)\n",
        "print(f\"✅ Saved postseason year-level rosters to {out_path} (rows: {len(out):,})\")\n",
        "\n",
        "# peek a few lines\n",
        "print(out.head(20).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448,
          "referenced_widgets": [
            "e85d9ad1e7804460b06c006f4fffa9c9",
            "e9f674a2c84b4601a762de95f4d8cb34",
            "8146e534489c4d9a9ae62dacfa4157af",
            "fefbc69e3fb64ef8b805fe3e650e06f9",
            "3969c10636c540468fc0124ec72c3e9b",
            "2c0dc3e514b3458fbf9d82148763298b",
            "e8fe91c1d2084448889979466257271b",
            "c2b75866750947d58553e46b66db47c1",
            "47a2b1a43eee4f2294bc93157f7be1a5",
            "4b2c1a0a98374c40ade7254c6422b48e",
            "1a77baaa3a98493398ab06b1976dd7d3"
          ]
        },
        "id": "mKIomSjVfgoO",
        "outputId": "397d118e-a759-489b-be03-1582de22f9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching boxscores & unioning to year-level rosters…\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/374 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e85d9ad1e7804460b06c006f4fffa9c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved postseason year-level rosters to /content/mlb_postseason_year_team_rosters_2014_2024.csv (rows: 2,871)\n",
            " year  teamId team  mlb_player_id         fullName primaryNumber position_code     position_name\n",
            " 2014     110  BAL         430945       Adam Jones            10             8        Outfielder\n",
            " 2014     110  BAL         457477 Alejandro De Aza            12             7        Outfielder\n",
            " 2014     110  BAL         453192    Andrew Miller            48             1           Pitcher\n",
            " 2014     110  BAL         542960       Brad Brach            35             1           Pitcher\n",
            " 2014     110  BAL         451085     Brian Matusz            17             1           Pitcher\n",
            " 2014     110  BAL         502032       Bud Norris            25             1           Pitcher\n",
            " 2014     110  BAL         543376     Caleb Joseph            36             2           Catcher\n",
            " 2014     110  BAL         501957    Chris Tillman            30             1           Pitcher\n",
            " 2014     110  BAL         503285     Darren O'Day            56             1           Pitcher\n",
            " 2014     110  BAL         518953      David Lough             9             7        Outfielder\n",
            " 2014     110  BAL         430321     Delmon Young            27             7        Outfielder\n",
            " 2014     110  BAL         429666       J.J. Hardy             2             6         Shortstop\n",
            " 2014     110  BAL         517370    Jimmy Paredes            38             5        Third Base\n",
            " 2014     110  BAL         570731  Jonathan Schoop             6             4       Second Base\n",
            " 2014     110  BAL         430637    Kelly Johnson            14             5        Third Base\n",
            " 2014     110  BAL         592332    Kevin Gausman            39             1           Pitcher\n",
            " 2014     110  BAL         456068  Miguel González            50             1           Pitcher\n",
            " 2014     110  BAL         443558      Nelson Cruz            23            10 Designated Hitter\n",
            " 2014     110  BAL         460026     Nick Hundley            40             2           Catcher\n",
            " 2014     110  BAL         455976    Nick Markakis            21             9        Outfielder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Win Loss identification\n",
        "INPUT_CSV  = \"/content/your_data_FINAL_cleaned.csv\"\n",
        "OUTPUT_CSV = \"/content/your_data_with_team_won.csv\"\n",
        "\n",
        "# === CODE ===\n",
        "import pandas as pd, numpy as np, re, time, requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------- 1) Load & clean your data --------\n",
        "df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False)\n",
        "\n",
        "# Drop rows with no team (execs/umpires)\n",
        "df[\"Team\"] = df[\"Team\"].str.strip()\n",
        "df[\"Team\"].replace({\"\": np.nan}, inplace=True)\n",
        "df = df.dropna(subset=[\"Team\"]).copy()\n",
        "\n",
        "# Safe year extraction (handles accidental text in the column)\n",
        "def safe_year(x):\n",
        "    m = re.search(r\"(20[0-3]\\d)\", str(x))\n",
        "    return int(m.group(1)) if m else np.nan\n",
        "\n",
        "df[\"year\"] = df[\"year\"].apply(safe_year)\n",
        "df = df.dropna(subset=[\"year\"]).copy()\n",
        "df[\"year\"] = df[\"year\"].astype(int)\n",
        "\n",
        "# Parse dates; your screenshot showed MM/DD/YY\n",
        "date_col = \"date_iso\" if \"date_iso\" in df.columns else \"date\"\n",
        "def parse_mdy2(x):\n",
        "    try:\n",
        "        return pd.to_datetime(x, format=\"%m/%d/%y\", errors=\"coerce\")\n",
        "    except Exception:\n",
        "        return pd.to_datetime(x, errors=\"coerce\")\n",
        "df[\"date\"] = df[date_col].apply(parse_mdy2)\n",
        "\n",
        "# Normalize team codes (to Retrosheet-style)\n",
        "TEAM_ALIASES = {\n",
        "    \"ANA\":\"LAA\",\"FLA\":\"MIA\",\"TBD\":\"TBA\",\"TB\":\"TBA\",\"WSH\":\"WAS\",\n",
        "    \"LAD\":\"LAN\",\"SDP\":\"SDN\",\"SFG\":\"SFN\",\"CHC\":\"CHN\",\"STL\":\"SLN\",\n",
        "    \"NYY\":\"NYA\",\"NYM\":\"NYN\",\"KC\":\"KCA\",\"SF\":\"SFN\",\"LA\":\"LAN\"\n",
        "}\n",
        "def norm_team(code):\n",
        "    if pd.isna(code): return np.nan\n",
        "    code = str(code).strip().upper()\n",
        "    return TEAM_ALIASES.get(code, code)\n",
        "df[\"Team\"] = df[\"Team\"].map(norm_team)\n",
        "\n",
        "VALID_YEARS = {2014,2015,2016,2017,2018,2019,2021,2022,2023,2024}\n",
        "df = df[df[\"year\"].isin(VALID_YEARS)].copy()\n",
        "\n",
        "# -------- 2) Build postseason Win/Loss lookup from MLB StatsAPI --------\n",
        "STATS_BASE = \"https://statsapi.mlb.com/api/v1\"\n",
        "HEADERS = {\"User-Agent\":\"Mozilla/5.0 (team-won/solid-1.0)\"}\n",
        "\n",
        "# MLB teamId -> Retrosheet-style 3-letter code\n",
        "TEAMID_TO_RETRO = {\n",
        "    110:\"BAL\",111:\"BOS\",147:\"NYA\",139:\"TBA\",141:\"TOR\",\n",
        "    145:\"CHA\",114:\"CLE\",116:\"DET\",118:\"KCA\",142:\"MIN\",\n",
        "    117:\"HOU\",108:\"LAA\",133:\"OAK\",136:\"SEA\",140:\"TEX\",\n",
        "    144:\"ATL\",146:\"MIA\",121:\"NYN\",143:\"PHI\",120:\"WAS\",\n",
        "    112:\"CHN\",113:\"CIN\",158:\"MIL\",134:\"PIT\",138:\"SLN\",\n",
        "    109:\"ARI\",115:\"COL\",119:\"LAN\",135:\"SDN\",137:\"SFN\"\n",
        "}\n",
        "\n",
        "def get_json(url, params=None, retries=3, backoff=0.6):\n",
        "    last = None\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            r = requests.get(url, params=params, headers=HEADERS, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            time.sleep(backoff*(i+1))\n",
        "    raise last\n",
        "\n",
        "def game_results_for_year(year: int):\n",
        "    \"\"\"Return rows: year, date, team_code, result ('Win'/'Loss') for postseason games.\"\"\"\n",
        "    rows = []\n",
        "\n",
        "    # Preferred: official postseason schedule\n",
        "    data = None\n",
        "    try:\n",
        "        data = get_json(f\"{STATS_BASE}/schedule/postseason\", params={\"season\":year,\"sportId\":1})\n",
        "    except Exception:\n",
        "        data = None\n",
        "\n",
        "    # Fallback: schedule with explicit postseason gameTypes\n",
        "    if not data or not data.get(\"dates\"):\n",
        "        try:\n",
        "            data = get_json(f\"{STATS_BASE}/schedule\", params={\"season\":year,\"sportId\":1,\"gameTypes\":\"F,D,L,W\"})\n",
        "        except Exception:\n",
        "            data = {\"dates\":[]}\n",
        "\n",
        "    for d in data.get(\"dates\", []):\n",
        "        for g in d.get(\"games\", []):\n",
        "            if not g.get(\"gamePk\"):\n",
        "                continue\n",
        "            # extract IDs & date\n",
        "            date = pd.to_datetime(g.get(\"officialDate\") or g.get(\"gameDate\",\"\")[:10], errors=\"coerce\")\n",
        "            home = g.get(\"teams\", {}).get(\"home\", {}) or {}\n",
        "            away = g.get(\"teams\", {}).get(\"away\", {}) or {}\n",
        "            hteam = (home.get(\"team\") or {}).get(\"id\")\n",
        "            ateam = (away.get(\"team\") or {}).get(\"id\")\n",
        "            if not (isinstance(hteam,int) and isinstance(ateam,int)):\n",
        "                continue\n",
        "            h_code = TEAMID_TO_RETRO.get(hteam)\n",
        "            a_code = TEAMID_TO_RETRO.get(ateam)\n",
        "            if not (h_code and a_code):\n",
        "                continue\n",
        "\n",
        "            # Preferred: use isWinner flags when present\n",
        "            h_win = home.get(\"isWinner\")\n",
        "            a_win = away.get(\"isWinner\")\n",
        "\n",
        "            if isinstance(h_win, bool) and isinstance(a_win, bool):\n",
        "                rows.append({\"year\":year, \"date\":date, \"team\":h_code, \"result\":\"Win\" if h_win else \"Loss\"})\n",
        "                rows.append({\"year\":year, \"date\":date, \"team\":a_code, \"result\":\"Win\" if a_win else \"Loss\"})\n",
        "                continue\n",
        "\n",
        "            # Fallback: pull linescore for final runs\n",
        "            try:\n",
        "                ls = get_json(f\"{STATS_BASE}/game/{int(g['gamePk'])}/linescore\")\n",
        "                teams = ls.get(\"teams\", {})\n",
        "                hr = teams.get(\"home\", {}).get(\"runs\")\n",
        "                ar = teams.get(\"away\", {}).get(\"runs\")\n",
        "                if hr is None or ar is None:\n",
        "                    continue\n",
        "                if hr > ar:\n",
        "                    rows.append({\"year\":year, \"date\":date, \"team\":h_code, \"result\":\"Win\"})\n",
        "                    rows.append({\"year\":year, \"date\":date, \"team\":a_code, \"result\":\"Loss\"})\n",
        "                elif ar > hr:\n",
        "                    rows.append({\"year\":year, \"date\":date, \"team\":a_code, \"result\":\"Win\"})\n",
        "                    rows.append({\"year\":year, \"date\":date, \"team\":h_code, \"result\":\"Loss\"})\n",
        "            except Exception:\n",
        "                # if even linescore fails, skip the game\n",
        "                continue\n",
        "\n",
        "    return rows\n",
        "\n",
        "years_needed = sorted(df[\"year\"].unique())\n",
        "all_rows = []\n",
        "for y in tqdm(years_needed, desc=\"Fetching postseason results\"):\n",
        "    if y == 2020:   # your dataset excludes 2020\n",
        "        continue\n",
        "    all_rows.extend(game_results_for_year(y))\n",
        "    time.sleep(0.2)\n",
        "\n",
        "results = pd.DataFrame(all_rows).dropna()\n",
        "results[\"date\"] = pd.to_datetime(results[\"date\"], errors=\"coerce\")\n",
        "\n",
        "# -------- 3) Merge & label --------\n",
        "merged = df.merge(results, left_on=[\"year\",\"Team\",\"date\"], right_on=[\"year\",\"team\",\"date\"], how=\"left\")\n",
        "merged[\"team_won\"] = merged[\"result\"].fillna(\"No Game\")\n",
        "merged.drop(columns=[c for c in [\"team\",\"result\"] if c in merged.columns], inplace=True)\n",
        "\n",
        "# -------- 4) Save --------\n",
        "merged.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"✅ Saved with Win/Loss labels: {OUTPUT_CSV}\")\n",
        "print(merged[\"team_won\"].value_counts(dropna=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "6cef2d6d2bb54097a1c0e3cf85fdd6cd",
            "f251d95d471044b687332337aed405b8",
            "38a4e582190d4eda9cb75834fd020d1a",
            "c66691f5bea64b1cbec088325b7cbaa2",
            "0ee6565dce6a4773b56352811446a3b9",
            "47dc00b943be44d0ae7dd2495d79b0e8",
            "9eb839ec8643424f8f627fad14afff77",
            "09c8da73125a49bba1771c12a0cc0772",
            "ea833f611ecc4d1eb9fc902adf1337c6",
            "a4ca41725b884e7b9e23a100ae00baf3",
            "b1016a1f9525461e87932e32fa14c056"
          ]
        },
        "id": "UZb7DUzNxDJP",
        "outputId": "02497a2d-cab4-4585-f498-99d932f7992e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2787576426.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"Team\"].replace({\"\": np.nan}, inplace=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching postseason results:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cef2d6d2bb54097a1c0e3cf85fdd6cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved with Win/Loss labels: /content/your_data_with_team_won.csv\n",
            "team_won\n",
            "Win        1639\n",
            "Loss       1144\n",
            "No Game     815\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deduplication, cleaning of transcripts + Word Count and Mentions\n",
        "import pandas as pd, numpy as np, re, unicodedata, os\n",
        "\n",
        "# ==== INPUTS (edit paths if needed) ====\n",
        "INTERVIEWS_IN = \"/content/your_data_with_team_won_games_only.xlsx\"\n",
        "TEAM_DATE_IN  = \"/content/team_date_teammate_mentions_with_series.csv\"  # use CSV if present, else switch to .xlsx\n",
        "\n",
        "# ==== OUTPUTS ====\n",
        "TEAM_DATE_OUT_CSV  = \"/content/team_date_teammate_mentions_with_series_cleanwords.csv\"\n",
        "TEAM_DATE_OUT_XLSX = \"/content/team_date_teammate_mentions_with_series_cleanwords.xlsx\"\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str): return \"\"\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.replace(\"\\xa0\", \" \").replace(\"Â\", \"\")\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
        "    s = s.replace(\"\\r\", \"\\n\")\n",
        "    return s.strip()\n",
        "\n",
        "def strip_questions_and_keep_interviewee(text: str, interviewee_name: str) -> str:\n",
        "    \"\"\"Remove Q./MODERATOR blocks and keep spans after INTERVIEWEE_LABEL: up to next label/Q/end.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    txt = normalize_text(text)\n",
        "    iname_up = (interviewee_name or \"\").strip().upper()\n",
        "    ilast = iname_up.split()[-1] if iname_up else \"\"\n",
        "\n",
        "    q_block = re.compile(r\"(?:^|\\s)Q[\\.:].*?(?=(?:\\s[A-Z][A-Z \\.'\\-]{1,40}:)|(?:\\sQ[\\.:])|$)\",\n",
        "                         flags=re.IGNORECASE | re.DOTALL)\n",
        "    mod_block = re.compile(r\"(?:^|\\s)(?:THE\\s+MODERATOR|MODERATOR)[\\.:].*?(?=(?:\\s[A-Z][A-Z \\.'\\-]{1,40}:)|(?:\\sQ[\\.:])|$)\",\n",
        "                           flags=re.IGNORECASE | re.DOTALL)\n",
        "    txt = re.sub(q_block, \" \", txt)\n",
        "    txt = re.sub(mod_block, \" \", txt)\n",
        "\n",
        "    if not iname_up:\n",
        "        return txt\n",
        "\n",
        "    alts = [re.escape(iname_up)]\n",
        "    if ilast: alts.append(re.escape(ilast))\n",
        "    label_alt = \"|\".join(alts)\n",
        "\n",
        "    iv_pat = re.compile(\n",
        "        rf\"(?:^|\\s)(?:{label_alt})\\s*:\\s*(.*?)(?=(?:\\s[A-Z][A-Z \\.'\\-]{{1,40}}:)|(?:\\sQ[\\.:])|$)\",\n",
        "        flags=re.IGNORECASE | re.DOTALL\n",
        "    )\n",
        "    pieces = [m.group(1).strip() for m in iv_pat.finditer(txt)]\n",
        "    iv_text = \" \".join(pieces).strip()\n",
        "    return iv_text if iv_text else txt\n",
        "\n",
        "def _normalize_sentence(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (s or \"\").strip()).lower()\n",
        "\n",
        "def dedupe_repetitions(text: str) -> str:\n",
        "    \"\"\"Remove repeated paragraphs & sentences; collapse doubled content.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    paras = [p.strip() for p in re.split(r\"\\n{2,}|\\r{2,}\", text) if p.strip()]\n",
        "    seen_p, keep_p = set(), []\n",
        "    for p in paras:\n",
        "        key = _normalize_sentence(p)\n",
        "        if key in seen_p: continue\n",
        "        seen_p.add(key); keep_p.append(p)\n",
        "    text1 = \"\\n\\n\".join(keep_p)\n",
        "    sents = [s.strip() for s in re.split(r'(?<=[\\.\\?\\!])\\s+', text1) if s.strip()]\n",
        "    seen_s, keep_s = set(), []\n",
        "    for s in sents:\n",
        "        key = _normalize_sentence(s)\n",
        "        if key in seen_s: continue\n",
        "        seen_s.add(key); keep_s.append(s)\n",
        "    return \" \".join(keep_s)\n",
        "\n",
        "def word_count_clean(text: str) -> int:\n",
        "    if not text: return 0\n",
        "    return len(re.findall(r\"\\b\\w+\\b\", text))\n",
        "\n",
        "# ---------- 1) Load interviews (source of clean words) ----------\n",
        "iv = pd.read_excel(INTERVIEWS_IN, dtype=str)\n",
        "for c in (\"year\",\"Team\",\"date\",\"interviewee\",\"transcript\"):\n",
        "    if c not in iv.columns: iv[c] = \"\"\n",
        "\n",
        "iv[\"year\"] = pd.to_numeric(iv[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "iv = iv[iv[\"year\"].notna()].copy()\n",
        "iv[\"year\"] = iv[\"year\"].astype(int)\n",
        "iv[\"date\"] = pd.to_datetime(iv[\"date\"], errors=\"coerce\")\n",
        "iv[\"interviewee\"] = iv[\"interviewee\"].fillna(\"\")\n",
        "iv[\"transcript\"]  = iv[\"transcript\"].fillna(\"\").map(normalize_text)\n",
        "\n",
        "# ---------- 2) Compute clean word counts per interview ----------\n",
        "def compute_clean_wc(row):\n",
        "    ans = strip_questions_and_keep_interviewee(row[\"transcript\"], row[\"interviewee\"])\n",
        "    ans = dedupe_repetitions(ans)\n",
        "    return word_count_clean(ans)\n",
        "\n",
        "wc_key = iv[[\"year\",\"date\",\"Team\",\"interviewee\",\"transcript\"]].copy()\n",
        "wc_key[\"word_count_clean\"] = wc_key.apply(compute_clean_wc, axis=1)\n",
        "\n",
        "# ---------- 3) Aggregate to team-date ----------\n",
        "wc_teamdate = (\n",
        "    wc_key.groupby([\"year\",\"date\",\"Team\"], as_index=False)\n",
        "          .agg(total_word_count_clean=(\"word_count_clean\",\"sum\"),\n",
        "               n_interviews_clean=(\"interviewee\",\"count\"))\n",
        ")\n",
        "\n",
        "# ---------- 4) Load your existing team file and merge clean words ----------\n",
        "if os.path.exists(TEAM_DATE_IN):\n",
        "    team_dt = pd.read_csv(TEAM_DATE_IN, dtype=str)\n",
        "else:\n",
        "    TEAM_DATE_IN = \"/content/team_date_teammate_mentions_with_series.xlsx\"\n",
        "    team_dt = pd.read_excel(TEAM_DATE_IN, dtype=str)\n",
        "\n",
        "team_dt[\"year\"] = pd.to_numeric(team_dt[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "team_dt[\"date\"] = pd.to_datetime(team_dt[\"date\"], errors=\"coerce\")\n",
        "team_dt[\"Team\"] = team_dt[\"Team\"].astype(str)\n",
        "\n",
        "merged = team_dt.merge(wc_teamdate, on=[\"year\",\"date\",\"Team\"], how=\"left\")\n",
        "\n",
        "# If you want a frequency using clean words:\n",
        "if \"total_teammate_mentions\" in merged.columns:\n",
        "    merged[\"mention_freq_clean\"] = (\n",
        "        pd.to_numeric(merged[\"total_teammate_mentions\"], errors=\"coerce\").fillna(0) /\n",
        "        merged[\"total_word_count_clean\"].replace({0:np.nan})\n",
        "    )\n",
        "\n",
        "# ---------- 5) Save updated team file with clean word counts ----------\n",
        "merged.to_csv(TEAM_DATE_OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "merged.to_excel(TEAM_DATE_OUT_XLSX, index=False)\n",
        "\n",
        "print(\"✅ Added clean word counts to team-date file:\")\n",
        "print(\"CSV :\", TEAM_DATE_OUT_CSV)\n",
        "print(\"XLSX:\", TEAM_DATE_OUT_XLSX)\n",
        "\n",
        "# quick peek\n",
        "keep_cols = [c for c in [\"year\",\"date\",\"Team\",\"total_teammate_mentions\",\"total_word_count\",\n",
        "                         \"total_word_count_clean\",\"mention_freq_clean\",\"n_interviews\",\"n_interviews_clean\",\n",
        "                         \"is_division_series\",\"is_championship_series\",\"is_world_series\",\"team_won\"]\n",
        "             if c in merged.columns]\n",
        "print(merged[keep_cols].head(10).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VknUt7O4fJBe",
        "outputId": "ab40f7be-57ea-4a9e-8b87-eb757781b0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Added clean word counts to team-date file:\n",
            "CSV : /content/team_date_teammate_mentions_with_series_cleanwords.csv\n",
            "XLSX: /content/team_date_teammate_mentions_with_series_cleanwords.xlsx\n",
            " year       date Team total_teammate_mentions total_word_count  total_word_count_clean  mention_freq_clean n_interviews  n_interviews_clean is_division_series is_championship_series is_world_series team_won\n",
            " 2014 2014-10-02  BAL                      26             8603                    2970            0.008754            4                   4                  1                      0               0      Win\n",
            " 2014 2014-10-02  DET                      26             6007                    1980            0.013131            3                   3                  1                      0               0     Loss\n",
            " 2014 2014-10-02  KCA                      50             9544                    3603            0.013877            4                   4                  1                      0               0      Win\n",
            " 2014 2014-10-02  LAA                      39             8781                    3809            0.010239            3                   3                  1                      0               0     Loss\n",
            " 2014 2014-10-03  BAL                      18             8503                    2598            0.006928            4                   4                  1                      0               0      Win\n",
            " 2014 2014-10-03  DET                      15             5582                    1617            0.009276            3                   3                  1                      0               0     Loss\n",
            " 2014 2014-10-03  KCA                      19             9993                    4028            0.004717            4                   4                  1                      0               0      Win\n",
            " 2014 2014-10-03  LAA                      28             8299                    3567            0.007850            3                   3                  1                      0               0     Loss\n",
            " 2014 2014-10-03  LAN                      23             9464                    3637            0.006324            3                   3                  1                      0               0     Loss\n",
            " 2014 2014-10-03  SFN                      32             9238                    3540            0.009040            4                   4                  1                      0               0      Win\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Team stats\n",
        "# === Setup ===\n",
        "!pip -q install pybaseball==2.2.7 openpyxl rapidfuzz\n",
        "\n",
        "import re, pandas as pd\n",
        "from rapidfuzz import process, fuzz\n",
        "from functools import lru_cache\n",
        "from google.colab import files\n",
        "from pybaseball import team_batting, team_pitching, schedule_and_record\n",
        "\n",
        "print(\"Upload your Excel (must include 'year' and a team column named 'Team' or 'team').\")\n",
        "uploaded = files.upload()\n",
        "xlsx_path = list(uploaded.keys())[0]\n",
        "\n",
        "df_in = pd.read_excel(xlsx_path)\n",
        "lower_cols = {c: c.strip().lower() for c in df_in.columns}\n",
        "df_in.rename(columns=lower_cols, inplace=True)\n",
        "team_col = \"team\" if \"team\" in df_in.columns else (\"Team\" if \"Team\" in df_in.columns else None)\n",
        "if team_col is None: team_col = \"team\"\n",
        "assert \"year\" in df_in.columns and team_col in df_in.columns, \"Need 'year' and 'Team' (or 'team').\"\n",
        "\n",
        "orig_cols = list(df_in.columns)\n",
        "df_in = df_in[df_in[\"year\"].between(2014, 2024)]\n",
        "df_in = df_in[df_in[\"year\"] != 2020].copy()\n",
        "df_in[team_col] = df_in[team_col].astype(str).str.strip().str.upper()\n",
        "\n",
        "# === Normalize team codes ===\n",
        "CANON = {\"BAL\":\"BAL\",\"BOS\":\"BOS\",\"NYY\":\"NYY\",\"TBR\":\"TBR\",\"TOR\":\"TOR\",\n",
        "         \"CHW\":\"CHW\",\"CLE\":\"CLE\",\"DET\":\"DET\",\"KCR\":\"KCR\",\"MIN\":\"MIN\",\n",
        "         \"HOU\":\"HOU\",\"LAA\":\"LAA\",\"OAK\":\"OAK\",\"SEA\":\"SEA\",\"TEX\":\"TEX\",\n",
        "         \"ATL\":\"ATL\",\"MIA\":\"MIA\",\"NYM\":\"NYM\",\"PHI\":\"PHI\",\"WSN\":\"WSN\",\n",
        "         \"CHC\":\"CHC\",\"CIN\":\"CIN\",\"MIL\":\"MIL\",\"PIT\":\"PIT\",\"STL\":\"STL\",\n",
        "         \"ARI\":\"ARI\",\"COL\":\"COL\",\"LAD\":\"LAD\",\"SDP\":\"SDP\",\"SFG\":\"SFG\"}\n",
        "ALT = {\"WAS\":\"WSN\",\"WSH\":\"WSN\",\"TAM\":\"TBR\",\"TB\":\"TBR\",\"ANA\":\"LAA\",\n",
        "       \"CWS\":\"CHW\",\"CHA\":\"CHW\",\"NYA\":\"NYY\",\"NYN\":\"NYM\",\n",
        "       \"SD\":\"SDP\",\"SDN\":\"SDP\",\"SF\":\"SFG\",\"SFN\":\"SFG\",\n",
        "       \"KC\":\"KCR\",\"KCA\":\"KCR\",\"CLV\":\"CLE\",\"LA\":\"LAD\",\"LAN\":\"LAD\",\n",
        "       \"SLN\":\"STL\"}\n",
        "\n",
        "ALLOWED = list(CANON.values())\n",
        "def normalize_code(raw):\n",
        "    x = str(raw).strip().upper()\n",
        "    if x in CANON: return CANON[x]\n",
        "    if x in ALT: return ALT[x]\n",
        "    best, score, _ = process.extractOne(x, ALLOWED, scorer=fuzz.WRatio)\n",
        "    return best if score >= 80 else x\n",
        "\n",
        "df_in[\"FG_CODE\"] = df_in[team_col].apply(normalize_code)\n",
        "\n",
        "# === Data fetchers ===\n",
        "@lru_cache(None)\n",
        "def fg_batting(year: int) -> pd.DataFrame:\n",
        "    tb = team_batting(year)\n",
        "    keep = {'Team','Season','R','OBP','Off','Def','WAR','wRC+'}\n",
        "    tb = tb[[c for c in tb.columns if c in keep]].copy()\n",
        "    tb = tb.rename(columns={'Team':'FG_CODE','Season':'year','R':'Runs Scored',\n",
        "                            'WAR':'Offensive WAR','Def':'Defensive WAR','wRC+':'OPS+'})\n",
        "    tb['year'] = tb['year'].astype(int)\n",
        "    return tb\n",
        "\n",
        "@lru_cache(None)\n",
        "def fg_pitching(year: int) -> pd.DataFrame:\n",
        "    tp = team_pitching(year)\n",
        "    keep = [c for c in tp.columns if c in ['Team','Season','IP','ERA-','FIP','R','WAR']]\n",
        "    tp = tp[keep].copy()\n",
        "    tp.rename(columns={'Team':'FG_CODE','Season':'year','WAR':'Pitching WAR'}, inplace=True)\n",
        "    tp['year'] = tp['year'].astype(int)\n",
        "    tp['Runs Against'] = pd.to_numeric(tp['R'], errors='coerce').round().astype('Int64')\n",
        "    tp['ERA+'] = tp['ERA-'].apply(lambda x: round(10000/x) if pd.notnull(x) and x>0 else pd.NA).astype('Int64')\n",
        "    return tp[['year','FG_CODE','FIP','Runs Against','ERA+','Pitching WAR']]\n",
        "\n",
        "def win_pct_from_schedule(code, year):\n",
        "    try:\n",
        "        s = schedule_and_record(code, year)\n",
        "    except Exception:\n",
        "        return pd.NA\n",
        "    w = l = None\n",
        "    if 'W' in s.columns and 'L' in s.columns:\n",
        "        w = pd.to_numeric(s['W'], errors='coerce').dropna().max()\n",
        "        l = pd.to_numeric(s['L'], errors='coerce').dropna().max()\n",
        "    elif 'W-L' in s.columns:\n",
        "        last = s['W-L'].dropna().astype(str).iloc[-1]\n",
        "        m = re.match(r'(\\d+)-(\\d+)', last)\n",
        "        if m: w, l = int(m.group(1)), int(m.group(2))\n",
        "    return round(w/(w+l),3) if w and l else pd.NA\n",
        "\n",
        "# === Merge all ===\n",
        "years = sorted(df_in['year'].unique())\n",
        "fg_bat_all = pd.concat([fg_batting(y) for y in years], ignore_index=True)\n",
        "fg_pit_all = pd.concat([fg_pitching(y) for y in years], ignore_index=True)\n",
        "fg_merged = pd.merge(fg_bat_all, fg_pit_all, on=['year','FG_CODE'], how='outer')\n",
        "\n",
        "cols = ['year','FG_CODE','OPS+','ERA+','FIP','Runs Scored','Runs Against',\n",
        "        'OBP','Offensive WAR','Pitching WAR','Defensive WAR']\n",
        "fg_merged = fg_merged[cols]\n",
        "\n",
        "out = df_in.merge(fg_merged, on=['year','FG_CODE'], how='left')\n",
        "# Win%\n",
        "win_cache = {(y, c): win_pct_from_schedule(c, int(y)) for y,c in out[['year','FG_CODE']].drop_duplicates().itertuples(index=False)}\n",
        "out['Win%'] = [win_cache.get((int(y), c), pd.NA) for y,c in out[['year','FG_CODE']].itertuples(index=False)]\n",
        "\n",
        "metric_cols = ['OPS+','ERA+','FIP','Runs Scored','Runs Against','Win%','OBP',\n",
        "               'Offensive WAR','Pitching WAR','Defensive WAR']\n",
        "out = out[orig_cols + [c for c in metric_cols if c not in orig_cols]]\n",
        "\n",
        "# === Save result ===\n",
        "out_path = \"mlb_enriched_from_your_sheet.xlsx\"\n",
        "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as w:\n",
        "    out.to_excel(w, index=False, sheet_name=\"enriched\")\n",
        "    df_in[['year', team_col, 'FG_CODE']].to_excel(w, index=False, sheet_name=\"audit_codes\")\n",
        "\n",
        "print(\"✅ Done! Downloading your enriched file...\")\n",
        "files.download(out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "_uhS0Sp7pz9b",
        "outputId": "31797f7a-7c50-49e1-98c4-2969147669b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your Excel (must include 'year' and a team column named 'Team' or 'team').\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-98e51cd8-660b-45da-8f32-5667e1fc7b65\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-98e51cd8-660b-45da-8f32-5667e1fc7b65\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving team_date_teammate_mentions_with_series_cleanwords.xlsx to team_date_teammate_mentions_with_series_cleanwords.xlsx\n",
            "✅ Done! Downloading your enriched file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ba135434-4ea5-4831-ae3b-383057255008\", \"mlb_enriched_from_your_sheet.xlsx\", 86953)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Win Percentage\n",
        "\n",
        "# --- Robust fix for Win% column ---\n",
        "from pybaseball import team_pitching\n",
        "from rapidfuzz import process, fuzz\n",
        "\n",
        "# 1️⃣ Rebuild FG_CODE if missing\n",
        "if 'FG_CODE' not in out.columns:\n",
        "    print(\"No FG_CODE column found — regenerating from team abbreviations...\")\n",
        "    CANON = {\n",
        "        \"BAL\":\"BAL\",\"BOS\":\"BOS\",\"NYY\":\"NYY\",\"TBR\":\"TBR\",\"TOR\":\"TOR\",\n",
        "        \"CHW\":\"CHW\",\"CLE\":\"CLE\",\"DET\":\"DET\",\"KCR\":\"KCR\",\"MIN\":\"MIN\",\n",
        "        \"HOU\":\"HOU\",\"LAA\":\"LAA\",\"OAK\":\"OAK\",\"SEA\":\"SEA\",\"TEX\":\"TEX\",\n",
        "        \"ATL\":\"ATL\",\"MIA\":\"MIA\",\"NYM\":\"NYM\",\"PHI\":\"PHI\",\"WSN\":\"WSN\",\n",
        "        \"CHC\":\"CHC\",\"CIN\":\"CIN\",\"MIL\":\"MIL\",\"PIT\":\"PIT\",\"STL\":\"STL\",\n",
        "        \"ARI\":\"ARI\",\"COL\":\"COL\",\"LAD\":\"LAD\",\"SDP\":\"SDP\",\"SFG\":\"SFG\"\n",
        "    }\n",
        "    ALT = {\"WAS\":\"WSN\",\"WSH\":\"WSN\",\"TAM\":\"TBR\",\"TB\":\"TBR\",\"TBA\":\"TBR\",\"ANA\":\"LAA\",\n",
        "           \"CWS\":\"CHW\",\"CHA\":\"CHW\",\"CHN\":\"CHC\",\"NYA\":\"NYY\",\"NYN\":\"NYM\",\n",
        "           \"SD\":\"SDP\",\"SDN\":\"SDP\",\"SF\":\"SFG\",\"SFN\":\"SFG\",\n",
        "           \"KC\":\"KCR\",\"KCA\":\"KCR\",\"CLV\":\"CLE\",\"LA\":\"LAD\",\"LAN\":\"LAD\",\"SLN\":\"STL\"}\n",
        "    allowed = list(CANON.values())\n",
        "    def normalize_code(raw):\n",
        "        x = str(raw).strip().upper()\n",
        "        if x in CANON: return CANON[x]\n",
        "        if x in ALT: return ALT[x]\n",
        "        best, score, _ = process.extractOne(x, allowed, scorer=fuzz.WRatio)\n",
        "        return best if score >= 80 else x\n",
        "\n",
        "    team_col = next((c for c in out.columns if c.lower() == 'team'), None)\n",
        "    out['FG_CODE'] = out[team_col].apply(normalize_code)\n",
        "\n",
        "# 2️⃣ Collect win/loss data from FanGraphs\n",
        "@lru_cache(None)\n",
        "def fg_team_records(year: int) -> pd.DataFrame:\n",
        "    df = team_pitching(year)\n",
        "    df = df.rename(columns={'Team': 'FG_CODE'})\n",
        "    df['year'] = year\n",
        "\n",
        "    # Locate win/loss columns safely\n",
        "    win_cols = [c for c in df.columns if c.lower() in ('w', 'wins')]\n",
        "    loss_cols = [c for c in df.columns if c.lower() in ('l', 'losses', 'los')]\n",
        "    if not win_cols or not loss_cols:\n",
        "        raise ValueError(f\"No W/L columns found for {year}: {df.columns.tolist()}\")\n",
        "\n",
        "    w_col, l_col = win_cols[0], loss_cols[0]\n",
        "    df['Win%'] = (df[w_col] / (df[w_col] + df[l_col])).round(3)\n",
        "    return df[['year', 'FG_CODE', 'Win%']]\n",
        "\n",
        "years = sorted(out['year'].dropna().unique().astype(int))\n",
        "fg_records = pd.concat([fg_team_records(y) for y in years], ignore_index=True)\n",
        "\n",
        "# 3️⃣ Merge Win% cleanly\n",
        "out = out.drop(columns=['Win%'], errors='ignore') \\\n",
        "         .merge(fg_records, on=['year', 'FG_CODE'], how='left')\n",
        "\n",
        "# 4️⃣ Save new version\n",
        "out_path = \"mlb_enriched_with_winpct_fixed_finalpls.xlsx\"\n",
        "out.to_excel(out_path, index=False)\n",
        "print(\"✅ Win% column successfully added and saved.\")\n",
        "files.download(out_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Tm6YTXeKvOrs",
        "outputId": "fbb30270-9f1b-4504-80ff-acafc2213a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Win% column successfully added and saved.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bd90289a-b2a9-4105-a6f8-87ef7d0ad584\", \"mlb_enriched_with_winpct_fixed_finalpls.xlsx\", 82254)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Home Field Advantage\n",
        "# === USER SETTINGS ===\n",
        "INPUT_XLSX  = \"mlb_enriched_with_winpct_fixed_finalpls.xlsx\"   # your dataset with 'date' and 'team'\n",
        "SHEET_NAME  = 0                              # or \"Sheet1\"\n",
        "DATE_COL    = \"date\"\n",
        "TEAM_COL    = \"team\"\n",
        "OUTPUT_XLSX = \"/content/mlb_playoffs_with_hfa.xlsx\"\n",
        "\n",
        "YEARS = [y for y in range(2014, 2025) if y != 2020]  # 2014–2024, skip 2020\n",
        "\n",
        "# === CODE ===\n",
        "import os, zipfile, pandas as pd, requests\n",
        "from datetime import timedelta\n",
        "\n",
        "ZIP_URL = \"https://www.retrosheet.org/downloads/postseason.zip\"\n",
        "ZIP_PATH = \"/content/postseason.zip\"\n",
        "EXTRACT_DIR = \"/content/retrosheet_postseason\"\n",
        "\n",
        "# ---- helpers ----\n",
        "def normalize_date_any(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"Convert to date-only; handles YYYY-MM-DD, YYYY/MM/DD, datetimes, and Retrosheet YYYYMMDD.\"\"\"\n",
        "    s = series.astype(str).str.strip()\n",
        "    out = pd.Series(pd.NaT, index=s.index, dtype=\"datetime64[ns]\")\n",
        "    m8 = s.str.fullmatch(r\"\\d{8}\")\n",
        "    if m8.any():\n",
        "        out.loc[m8] = pd.to_datetime(s.loc[m8], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "    rest = ~m8\n",
        "    if rest.any():\n",
        "        out.loc[rest] = pd.to_datetime(s.loc[rest], errors=\"coerce\")\n",
        "    return out.dt.normalize()\n",
        "\n",
        "# Map both sides to a single canonical code so they match.\n",
        "# Your sample uses Retrosheet-style codes like KCA, LAN, SFN, SLN, WAS, LAA.\n",
        "# The only mismatch we saw was Angels as ANA (Retrosheet) vs LAA (your sheet).\n",
        "TEAM_ALIAS_TO_CANON = {\n",
        "    # Nationals\n",
        "    \"WSH\": \"WAS\",\n",
        "    # Angels\n",
        "    \"ANA\": \"LAA\",\n",
        "    # (Add more if you find mismatches; examples below commented out)\n",
        "    # \"TB\": \"TBA\",\n",
        "    # \"KC\": \"KCA\",\n",
        "    # \"LAD\": \"LAN\",\n",
        "    # \"SFG\": \"SFN\",\n",
        "    # \"STL\": \"SLN\",\n",
        "    # \"CHC\": \"CHN\",\n",
        "    # \"CHW\": \"CHA\",\n",
        "}\n",
        "\n",
        "def canon(code: str) -> str:\n",
        "    c = str(code).strip().upper()\n",
        "    return TEAM_ALIAS_TO_CANON.get(c, c)\n",
        "\n",
        "# ---- download & load Retrosheet postseason ----\n",
        "if not os.path.exists(ZIP_PATH):\n",
        "    r = requests.get(ZIP_URL, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    with open(ZIP_PATH, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "if not os.path.exists(EXTRACT_DIR):\n",
        "    os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "        z.extractall(EXTRACT_DIR)\n",
        "\n",
        "gi = pd.read_csv(os.path.join(EXTRACT_DIR, \"gameinfo.csv\"), dtype=str)\n",
        "\n",
        "required = [\"visteam\",\"hometeam\",\"date\",\"season\"]\n",
        "missing = [c for c in required if c not in gi.columns]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing expected columns in gameinfo.csv: {missing}\")\n",
        "\n",
        "gi[\"season\"] = pd.to_numeric(gi[\"season\"], errors=\"coerce\")\n",
        "gi = gi[gi[\"season\"].isin(YEARS)].copy()\n",
        "\n",
        "# Exclude Wild Card via any available descriptor columns\n",
        "wc_cols = [c for c in [\"gametype\",\"round\",\"series\",\"ser\",\"sername\"] if c in gi.columns]\n",
        "if wc_cols:\n",
        "    wc_mask = False\n",
        "    for c in wc_cols:\n",
        "        s = gi[c].astype(str).str.upper()\n",
        "        wc_mask = wc_mask | s.str.contains(\"WILD\", na=False) | s.str.contains(r\"\\bWC\\b\", regex=True, na=False)\n",
        "    gi = gi[~wc_mask].copy()\n",
        "\n",
        "# Normalize dates & teams (to canonical)\n",
        "gi[\"_date_norm\"] = normalize_date_any(gi[\"date\"])\n",
        "gi[\"_home_team_norm\"] = gi[\"hometeam\"].map(canon)\n",
        "gi[\"_away_team_norm\"] = gi[\"visteam\"].map(canon)\n",
        "\n",
        "home_rows = gi[[\"_date_norm\",\"_home_team_norm\"]].rename(columns={\"_home_team_norm\":\"_team_norm\"})\n",
        "home_rows[\"home_field_advantage\"] = 1\n",
        "away_rows = gi[[\"_date_norm\",\"_away_team_norm\"]].rename(columns={\"_away_team_norm\":\"_team_norm\"})\n",
        "away_rows[\"home_field_advantage\"] = 0\n",
        "postseason_team_dates = pd.concat([home_rows, away_rows], ignore_index=True)\n",
        "\n",
        "# ---- load your file ----\n",
        "df = pd.read_excel(INPUT_XLSX, sheet_name=SHEET_NAME)\n",
        "if DATE_COL not in df.columns or TEAM_COL not in df.columns:\n",
        "    raise ValueError(f\"Expected columns '{DATE_COL}' and '{TEAM_COL}'. Found: {list(df.columns)}\")\n",
        "\n",
        "df[\"_date_norm\"] = normalize_date_any(df[DATE_COL])\n",
        "df[\"_team_norm\"] = df[TEAM_COL].map(canon)\n",
        "\n",
        "# ---- first-pass merge (exact date) ----\n",
        "merged = df.merge(\n",
        "    postseason_team_dates,\n",
        "    on=[\"_date_norm\",\"_team_norm\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# ---- second-pass fix: SAFE ±1 day retry (unambiguous only) ----\n",
        "USE_SAFE_FUZZY = True  # set False to disable fuzzy matching entirely\n",
        "\n",
        "if USE_SAFE_FUZZY:\n",
        "    # Build quick lookups to test ambiguity\n",
        "    # Count how many entries exist for a (date, team) key in postseason data\n",
        "    key_counts = postseason_team_dates.groupby([\"_date_norm\",\"_team_norm\"]).size().rename(\"cnt\").reset_index()\n",
        "\n",
        "    # mark rows still missing\n",
        "    needs_fix = merged[\"home_field_advantage\"].isna()\n",
        "    if needs_fix.any():\n",
        "        # subset of keys that failed exact match\n",
        "        fix_keys = merged.loc[needs_fix, [\"_date_norm\",\"_team_norm\"]].copy()\n",
        "\n",
        "        # Helper to try a shifted day and return a Series of results,\n",
        "        # but only where the (date_shift, team) key exists exactly once.\n",
        "        def try_shift(delta_days: int, label: str):\n",
        "            tmp = fix_keys.copy()\n",
        "            tmp[\"_date_norm_shift\"] = tmp[\"_date_norm\"] + pd.Timedelta(days=delta_days)\n",
        "\n",
        "            # join to counts to ensure unambiguous match (exactly one)\n",
        "            tmp = tmp.merge(\n",
        "                key_counts.rename(columns={\"_date_norm\":\"_date_norm_shift\"}),\n",
        "                on=[\"_date_norm_shift\",\"_team_norm\"],\n",
        "                how=\"left\"\n",
        "            )\n",
        "            tmp = tmp[tmp[\"cnt\"] == 1]  # keep only unique keys\n",
        "\n",
        "            # fetch the actual HFA for those keys\n",
        "            res = tmp.merge(\n",
        "                postseason_team_dates.rename(columns={\"_date_norm\":\"_date_norm_shift\"}),\n",
        "                on=[\"_date_norm_shift\",\"_team_norm\"],\n",
        "                how=\"left\"\n",
        "            )[[\"home_field_advantage\"]]\n",
        "\n",
        "            res = res.rename(columns={\"home_field_advantage\": f\"hfa_{label}\"})\n",
        "            res.index = tmp.index  # align back to fix_keys index positions\n",
        "            return res\n",
        "\n",
        "        # Try -1 day and +1 day\n",
        "        res_minus = try_shift(-1, \"minus\")\n",
        "        res_plus  = try_shift(+1, \"plus\")\n",
        "\n",
        "        # Combine back onto the original index for rows needing fix\n",
        "        candidates = pd.DataFrame(index=fix_keys.index)\n",
        "        if not res_minus.empty:\n",
        "            candidates = candidates.join(res_minus, how=\"left\")\n",
        "        if not res_plus.empty:\n",
        "            candidates = candidates.join(res_plus, how=\"left\")\n",
        "\n",
        "        # Decision rule:\n",
        "        # - If exactly one of hfa_minus / hfa_plus is non-null -> use it\n",
        "        # - If both non-null (ambiguous) or both null -> leave as NaN\n",
        "        if not candidates.empty:\n",
        "            choice = candidates.apply(\n",
        "                lambda r: r[\"hfa_minus\"]\n",
        "                          if pd.notna(r.get(\"hfa_minus\")) and pd.isna(r.get(\"hfa_plus\"))\n",
        "                          else (r[\"hfa_plus\"] if pd.notna(r.get(\"hfa_plus\")) and pd.isna(r.get(\"hfa_minus\"))\n",
        "                                else pd.NA),\n",
        "                axis=1\n",
        "            )\n",
        "            merged.loc[needs_fix, \"home_field_advantage\"] = merged.loc[needs_fix, \"home_field_advantage\"].fillna(choice)\n",
        "\n",
        "# ---- save ----\n",
        "out = merged.drop(columns=[\"_date_norm\",\"_team_norm\"])\n",
        "out.to_excel(OUTPUT_XLSX, index=False)\n",
        "\n",
        "unmatched = int(out[\"home_field_advantage\"].isna().sum())\n",
        "total = len(out)\n",
        "print(f\"✅ Done! Wrote: {OUTPUT_XLSX}\")\n",
        "print(f\"Remaining unmatched (NaN) after alias + ±1 day pass: {unmatched} / {total}\")\n",
        "\n",
        "# quick peek\n",
        "cols = [c for c in [DATE_COL, TEAM_COL, \"home_field_advantage\"] if c in out.columns]\n",
        "print(out[cols].head(25).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvNKkNJkOWp3",
        "outputId": "e717083d-d843-4010-d550-9ade42fb9798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done! Wrote: /content/mlb_playoffs_with_hfa.xlsx\n",
            "Remaining unmatched (NaN) after alias + ±1 day pass: 0 / 664\n",
            "      date team  home_field_advantage\n",
            "2014-10-02  BAL                     1\n",
            "2014-10-02  DET                     0\n",
            "2014-10-02  KCA                     0\n",
            "2014-10-02  LAA                     1\n",
            "2014-10-03  BAL                     1\n",
            "2014-10-03  DET                     0\n",
            "2014-10-03  KCA                     0\n",
            "2014-10-03  LAA                     1\n",
            "2014-10-03  LAN                     1\n",
            "2014-10-03  SFN                     0\n",
            "2014-10-03  SLN                     0\n",
            "2014-10-03  WAS                     1\n",
            "2014-10-04  LAN                     1\n",
            "2014-10-04  SFN                     0\n",
            "2014-10-04  SLN                     0\n",
            "2014-10-04  WAS                     1\n",
            "2014-10-05  BAL                     0\n",
            "2014-10-05  DET                     1\n",
            "2014-10-05  KCA                     1\n",
            "2014-10-05  LAA                     0\n",
            "2014-10-06  LAN                     0\n",
            "2014-10-06  SFN                     1\n",
            "2014-10-06  SLN                     1\n",
            "2014-10-06  WAS                     0\n",
            "2014-10-07  LAN                     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "suZzSuHlhCCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Momemntum - Sept Win Percentage\n",
        "# === USER SETTINGS ===\n",
        "INPUT_XLSX   = \"/content/mlb_playoffs_with_hfa.xlsx\"     # your current file (must have 'year' and 'team')\n",
        "SHEET_NAME   = 0                                          # or \"Sheet1\"\n",
        "YEAR_COL     = \"year\"\n",
        "TEAM_COL     = \"team\"\n",
        "OUTPUT_XLSX  = \"/content/mlb_playoffs_with_hfa_and_septwinpct.xlsx\"\n",
        "\n",
        "YEARS = [y for y in range(2014, 2025) if y != 2020]       # 2014–2024, skip 2020\n",
        "\n",
        "# === CODE ===\n",
        "import io, os, zipfile, pandas as pd, requests\n",
        "\n",
        "GL_BASE = \"https://www.retrosheet.org/gamelogs\"\n",
        "\n",
        "# --- Team code canonicalization ---\n",
        "# Map common user/MLB/BBR codes -> Retrosheet gamelog codes (default mapping)\n",
        "ALIAS_TO_RS_BASE = {\n",
        "    # AL East\n",
        "    \"NYY\":\"NYA\",\"NYA\":\"NYA\",\"BOS\":\"BOS\",\"BAL\":\"BAL\",\"TOR\":\"TOR\",\n",
        "    \"TBR\":\"TBA\",\"TB\":\"TBA\",\"TBA\":\"TBA\",\n",
        "    # AL Central\n",
        "    \"CHW\":\"CHA\",\"CWS\":\"CHA\",\"CHA\":\"CHA\",\"CLE\":\"CLE\",\"DET\":\"DET\",\n",
        "    \"KCR\":\"KCA\",\"KC\":\"KCA\",\"KCA\":\"KCA\",\"MIN\":\"MIN\",\n",
        "    # AL West\n",
        "    \"OAK\":\"OAK\",\"SEA\":\"SEA\",\"TEX\":\"TEX\",\"HOU\":\"HOU\",\n",
        "    \"LAA\":\"LAA\",\"ANA\":\"ANA\",  # handled year-aware below\n",
        "    # NL East\n",
        "    \"ATL\":\"ATL\",\"MIA\":\"MIA\",\"FLA\":\"MIA\",\"NYM\":\"NYN\",\"NYN\":\"NYN\",\"PHI\":\"PHI\",\n",
        "    \"WSN\":\"WAS\",\"WSH\":\"WAS\",\"WAS\":\"WAS\",\n",
        "    # NL Central\n",
        "    \"CHC\":\"CHN\",\"CHN\":\"CHN\",\"CIN\":\"CIN\",\"MIL\":\"MIL\",\"PIT\":\"PIT\",\n",
        "    \"STL\":\"SLN\",\"SLN\":\"SLN\",\n",
        "    # NL West\n",
        "    \"ARI\":\"ARI\",\"COL\":\"COL\",\n",
        "    \"LAD\":\"LAN\",\"LAN\":\"LAN\",\n",
        "    \"SDP\":\"SDN\",\"SD\":\"SDN\",\"SDN\":\"SDN\",\n",
        "    \"SFG\":\"SFN\",\"SFN\":\"SFN\",\n",
        "}\n",
        "\n",
        "def canon_team_year(code: str, year: int) -> str:\n",
        "    \"\"\"\n",
        "    Canonicalize a user team code to Retrosheet gamelog code, with year-aware handling for the Angels.\n",
        "    Retrosheet often uses ANA (2014–2015) and LAA later.\n",
        "    \"\"\"\n",
        "    c = str(code).strip().upper()\n",
        "    rs = ALIAS_TO_RS_BASE.get(c, c)\n",
        "    if c in (\"LAA\", \"ANA\"):            # Angels special-case by season\n",
        "        rs = \"ANA\" if int(year) <= 2015 else \"LAA\"\n",
        "    return rs\n",
        "\n",
        "def fetch_gamelog_year(year: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Download & read Retrosheet GL{year}.TXT; return needed fields.\n",
        "    \"\"\"\n",
        "    url = f\"{GL_BASE}/gl{year}.zip\"\n",
        "    zpath = f\"/content/gl{year}.zip\"\n",
        "\n",
        "    if not os.path.exists(zpath):\n",
        "        r = requests.get(url, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        with open(zpath, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "    with zipfile.ZipFile(zpath, \"r\") as z:\n",
        "        name = None\n",
        "        for n in z.namelist():\n",
        "            if n.upper().endswith(f\"GL{year}.TXT\"):\n",
        "                name = n\n",
        "                break\n",
        "        if name is None:\n",
        "            raise RuntimeError(f\"Could not find GL{year}.TXT in the zip.\")\n",
        "        with z.open(name) as f:\n",
        "            # Gamelog columns (0-based): 0=date(YYYYMMDD), 3=visteam, 6=hometeam, 9=v_score, 10=h_score\n",
        "            cols = [0,3,6,9,10]\n",
        "            df = pd.read_csv(f, header=None, usecols=cols, dtype=str)\n",
        "            df.columns = [\"date_raw\",\"visteam\",\"hometeam\",\"v_score\",\"h_score\"]\n",
        "\n",
        "    dt = pd.to_datetime(df[\"date_raw\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "    df = df.assign(\n",
        "        date = dt.dt.normalize(),\n",
        "        month = dt.dt.month,\n",
        "        season = dt.dt.year,\n",
        "        v_score = pd.to_numeric(df[\"v_score\"], errors=\"coerce\"),\n",
        "        h_score = pd.to_numeric(df[\"h_score\"], errors=\"coerce\")\n",
        "    )\n",
        "    print(f\"{year}: loaded September game logs\")\n",
        "    return df[[\"date\",\"month\",\"season\",\"visteam\",\"hometeam\",\"v_score\",\"h_score\"]]\n",
        "\n",
        "# --- Build September win% per (team, season) from 2014–2024 (skip 2020) ---\n",
        "parts = []\n",
        "for yr in YEARS:\n",
        "    try:\n",
        "        gl = fetch_gamelog_year(yr)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed {yr}: {e}\")\n",
        "        continue\n",
        "\n",
        "    sep = gl[gl[\"month\"] == 9].copy()\n",
        "\n",
        "    # two team-rows per game: visitor & home\n",
        "    away = sep[[\"season\",\"visteam\",\"v_score\",\"h_score\"]].copy()\n",
        "    away[\"team_rs\"] = away[\"visteam\"]\n",
        "    away[\"win\"] = (away[\"v_score\"] > away[\"h_score\"]).astype(int)\n",
        "\n",
        "    home = sep[[\"season\",\"hometeam\",\"v_score\",\"h_score\"]].copy()\n",
        "    home[\"team_rs\"] = home[\"hometeam\"]\n",
        "    home[\"win\"] = (home[\"h_score\"] > home[\"v_score\"]).astype(int)\n",
        "\n",
        "    team_rows = pd.concat([\n",
        "        away.rename(columns={\"visteam\":\"opp_rs\"})[[\"season\",\"team_rs\",\"win\"]],\n",
        "        home.rename(columns={\"hometeam\":\"opp_rs\"})[[\"season\",\"team_rs\",\"win\"]],\n",
        "    ], ignore_index=True)\n",
        "\n",
        "    agg = team_rows.groupby([\"season\",\"team_rs\"], as_index=False).agg(\n",
        "        sept_wins=(\"win\",\"sum\"),\n",
        "        sept_games=(\"win\",\"count\")\n",
        "    )\n",
        "    agg[\"sept_win_pct\"] = agg[\"sept_wins\"] / agg[\"sept_games\"]\n",
        "    parts.append(agg[[\"season\",\"team_rs\",\"sept_win_pct\"]])\n",
        "\n",
        "sept_win = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame(columns=[\"season\",\"team_rs\",\"sept_win_pct\"])\n",
        "\n",
        "# As a robustness layer, also add alias duplicates (so either code will match if needed)\n",
        "alias_pairs = [\n",
        "    (\"ANA\",\"LAA\"), (\"LAA\",\"ANA\"),\n",
        "    (\"WAS\",\"WSN\"), (\"TBA\",\"TBR\"),\n",
        "    (\"KCA\",\"KCR\"),\n",
        "    (\"CHN\",\"CHC\"), (\"CHA\",\"CHW\"),\n",
        "    (\"LAN\",\"LAD\"), (\"SFN\",\"SFG\"),\n",
        "    (\"SLN\",\"STL\"), (\"SDN\",\"SDP\"),\n",
        "    (\"NYN\",\"NYM\"),\n",
        "]\n",
        "exp = []\n",
        "for rs, user in alias_pairs:\n",
        "    tmp = sept_win[sept_win[\"team_rs\"] == rs].copy()\n",
        "    if not tmp.empty:\n",
        "        tmp[\"team_rs\"] = user\n",
        "        exp.append(tmp)\n",
        "if exp:\n",
        "    sept_win = pd.concat([sept_win] + exp, ignore_index=True)\n",
        "\n",
        "# --- Load your playoff file and map its (year, team) to Retrosheet codes (year-aware for Angels) ---\n",
        "df = pd.read_excel(INPUT_XLSX, sheet_name=SHEET_NAME)\n",
        "if YEAR_COL not in df.columns or TEAM_COL not in df.columns:\n",
        "    raise ValueError(f\"Expected columns '{YEAR_COL}' and '{TEAM_COL}' in your file.\")\n",
        "\n",
        "df[\"_season\"] = pd.to_numeric(df[YEAR_COL], errors=\"coerce\").astype(\"Int64\")\n",
        "df[\"_team_rs\"] = df.apply(lambda r: canon_team_year(r[TEAM_COL], r[YEAR_COL]), axis=1)\n",
        "\n",
        "# --- Merge and save ---\n",
        "merged = df.merge(\n",
        "    sept_win.rename(columns={\"season\":\"_season\",\"team_rs\":\"_team_rs\"}),\n",
        "    left_on=[\"_season\",\"_team_rs\"],\n",
        "    right_on=[\"_season\",\"_team_rs\"],\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "out = merged.drop(columns=[\"_season\",\"_team_rs\"])\n",
        "out.to_excel(OUTPUT_XLSX, index=False)\n",
        "\n",
        "missing = out[\"sept_win_pct\"].isna().sum()\n",
        "print(f\"✅ Done! Wrote: {OUTPUT_XLSX}\")\n",
        "print(f\"Rows without a September win% after merge: {missing} / {len(out)}\")\n",
        "print(out[[YEAR_COL, TEAM_COL, \"sept_win_pct\"]].head(20).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8q6dwbESMsG",
        "outputId": "82983f5a-1aff-412b-e1d0-2cc5f20ce02b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2014: loaded September game logs\n",
            "2015: loaded September game logs\n",
            "2016: loaded September game logs\n",
            "2017: loaded September game logs\n",
            "2018: loaded September game logs\n",
            "2019: loaded September game logs\n",
            "2021: loaded September game logs\n",
            "2022: loaded September game logs\n",
            "2023: loaded September game logs\n",
            "2024: loaded September game logs\n",
            "✅ Done! Wrote: /content/mlb_playoffs_with_hfa_and_septwinpct.xlsx\n",
            "Rows without a September win% after merge: 0 / 664\n",
            " year team  sept_win_pct\n",
            " 2014  BAL      0.629630\n",
            " 2014  DET      0.615385\n",
            " 2014  KCA      0.576923\n",
            " 2014  LAA      0.576923\n",
            " 2014  BAL      0.629630\n",
            " 2014  DET      0.615385\n",
            " 2014  KCA      0.576923\n",
            " 2014  LAA      0.576923\n",
            " 2014  LAN      0.680000\n",
            " 2014  SFN      0.520000\n",
            " 2014  SLN      0.653846\n",
            " 2014  WAS      0.703704\n",
            " 2014  LAN      0.680000\n",
            " 2014  SFN      0.520000\n",
            " 2014  SLN      0.653846\n",
            " 2014  WAS      0.703704\n",
            " 2014  BAL      0.629630\n",
            " 2014  DET      0.615385\n",
            " 2014  KCA      0.576923\n",
            " 2014  LAA      0.576923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Opponent Matching\n",
        "\n",
        "# === COLAB-READY: Add opponent team from Retrosheet ===\n",
        "# Works with CSV or Excel inputs. Handles multiple games per day and doubleheaders.\n",
        "\n",
        "l\n",
        "# --------------------------\n",
        "# 0) USER SETTINGS\n",
        "# --------------------------\n",
        "FILE_PATH = '/content/mlb_playoffs_with_hfa_and_septwinpct.xlsx'  # <- change to your file path\n",
        "OUT_PATH  = '/content/mlb_playoffs_with_opponents.csv'            # output CSV\n",
        "\n",
        "# Optional explicit column names (leave as None to auto-detect)\n",
        "DATE_COL = 'date'           # e.g., 'date' or 'game_date'\n",
        "TEAM_COL = 'team'          # e.g., 'team_abbr' or 'team'\n",
        "DH_COL   = None           # doubleheader game number column if you have it (0/1/2...). Leave None if not applicable.\n",
        "\n",
        "# Optional filter (uncomment to restrict to postseason only)\n",
        "POSTSEASON_ONLY = False   # True to restrict to postseason (Retrosheet 'gametype' includes 'post')\n",
        "\n",
        "# --------------------------\n",
        "# 1) LOAD YOUR DATAFRAME\n",
        "# --------------------------\n",
        "def load_df(path: str) -> pd.DataFrame:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found at: {path}\\n\"\n",
        "                                f\"Tip: in Colab, either upload the file or mount Google Drive and update FILE_PATH.\")\n",
        "    # Choose reader based on extension; fall back gracefully\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext in ('.xlsx', '.xls'):\n",
        "        return pd.read_excel(path)\n",
        "    elif ext in ('.csv', '.txt'):\n",
        "        # Try UTF-8 first, then Latin-1 (handles many “weird” CSVs)\n",
        "        try:\n",
        "            return pd.read_csv(path)\n",
        "        except UnicodeDecodeError:\n",
        "            return pd.read_csv(path, encoding='latin1')\n",
        "    else:\n",
        "        # Try Excel then CSV\n",
        "        try:\n",
        "            return pd.read_excel(path)\n",
        "        except Exception:\n",
        "            return pd.read_csv(path, low_memory=False)\n",
        "\n",
        "df = load_df(FILE_PATH)\n",
        "print(f\"Loaded dataset: {FILE_PATH}  shape={df.shape}\")\n",
        "# display(df.head())\n",
        "\n",
        "# --------------------------\n",
        "# 2) AUTO-DETECT CORE COLUMNS (unless explicitly provided)\n",
        "# --------------------------\n",
        "def pick(df, names, what):\n",
        "    if what in ('date', 'team', 'doubleheader'):\n",
        "        # Prefer exact matches first, then case-insensitive\n",
        "        for n in names:\n",
        "            if n in df.columns: return n\n",
        "        # case-insensitive fallback\n",
        "        lower = {c.lower(): c for c in df.columns}\n",
        "        for n in names:\n",
        "            if n.lower() in lower: return lower[n.lower()]\n",
        "    raise ValueError(f\"Could not find {what} column. Looked for: {names}\\nYour columns: {list(df.columns)}\")\n",
        "\n",
        "if DATE_COL is None:\n",
        "    DATE_COL = pick(df, ['date','game_date','Date','DATE','gameDate'], 'date')\n",
        "if TEAM_COL is None:\n",
        "    TEAM_COL = pick(df, ['team_abbr','team','Team','TEAM','tm','team_code','team_name','team_name_abbr'], 'team')\n",
        "\n",
        "if DH_COL is None:\n",
        "    for cand in ['game_number','gamenumber','number','dh','DH','doubleheader_game','game_no']:\n",
        "        if cand in df.columns:\n",
        "            DH_COL = cand\n",
        "            break\n",
        "\n",
        "print(f\"Detected columns → DATE_COL='{DATE_COL}', TEAM_COL='{TEAM_COL}', DH_COL='{DH_COL}'\")\n",
        "\n",
        "# --------------------------\n",
        "# 3) NORMALIZE DATES & TEAM LABELS\n",
        "# --------------------------\n",
        "df = df.copy()\n",
        "df[DATE_COL] = pd.to_datetime(df[DATE_COL], errors='coerce')\n",
        "if df[DATE_COL].isna().any():\n",
        "    bad = df[df[DATE_COL].isna()].head(10)\n",
        "    raise ValueError(f\"Some dates could not be parsed in '{DATE_COL}'. Example rows:\\n{bad}\")\n",
        "\n",
        "df['__date'] = df[DATE_COL].dt.date\n",
        "\n",
        "# Map your team labels → Retrosheet team codes (3 letters like NYA, LAN, CHN)\n",
        "# Extend this dictionary if your labels differ.\n",
        "to_retro = {\n",
        "    # AL East\n",
        "    'NYY':'NYA','NY':'NYA','YANKEES':'NYA','NYY ':'NYA',\n",
        "    'BOS':'BOS','RED SOX':'BOS',\n",
        "    'TBR':'TBA','TB':'TBA','RAYS':'TBA',\n",
        "    'TOR':'TOR','BLUE JAYS':'TOR',\n",
        "    'BAL':'BAL','ORIOLES':'BAL',\n",
        "    # AL Central\n",
        "    'CWS':'CHA','CHW':'CHA','WHITESOX':'CHA','WHITE SOX':'CHA',\n",
        "    'CLE':'CLE','GUARDIANS':'CLE','INDIANS':'CLE',\n",
        "    'DET':'DET','TIGERS':'DET',\n",
        "    'KCR':'KCA','KC':'KCA','ROYALS':'KCA',\n",
        "    'MIN':'MIN','TWINS':'MIN',\n",
        "    # AL West\n",
        "    'LAA':'ANA','ANGELS':'ANA','ANA':'ANA',\n",
        "    'HOU':'HOU','ASTROS':'HOU',\n",
        "    'OAK':'OAK','ATHLETICS':'OAK',\n",
        "    'SEA':'SEA','MARINERS':'SEA',\n",
        "    'TEX':'TEX','RANGERS':'TEX',\n",
        "    # NL East\n",
        "    'ATL':'ATL','BRAVES':'ATL',\n",
        "    'MIA':'FLO','FLA':'FLO','MARLINS':'FLO',\n",
        "    'NYM':'NYN','METS':'NYN',\n",
        "    'PHI':'PHI','PHILLIES':'PHI',\n",
        "    # Replace the Nationals mapping in your to_retro dict with:\n",
        "    'WAS':'WAS','WSH':'WAS','NATIONALS':'WAS','WASHINGTON':'WAS',\n",
        "\n",
        "    # NL Central\n",
        "    'CHC':'CHN','CUBS':'CHN',\n",
        "    'CIN':'CIN','REDS':'CIN',\n",
        "    'MIL':'MIL','BREWERS':'MIL',\n",
        "    'PIT':'PIT','PIRATES':'PIT',\n",
        "    'STL':'SLN','CARDINALS':'SLN','STL ':'SLN',\n",
        "    # NL West\n",
        "    'ARI':'ARI','ARZ':'ARI','D-BACKS':'ARI','DBACKS':'ARI','DIAMONDBACKS':'ARI',\n",
        "    'COL':'COL','ROCKIES':'COL',\n",
        "    'LAD':'LAN','DODGERS':'LAN',\n",
        "    'SDP':'SDN','SD':'SDN','PADRES':'SDN',\n",
        "    'SFG':'SFN','SF':'SFN','GIANTS':'SFN',\n",
        "}\n",
        "def norm_team_to_retro(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    s = str(x).upper().strip()\n",
        "    return to_retro.get(s, s)\n",
        "\n",
        "df['__team_retro'] = df[TEAM_COL].apply(norm_team_to_retro)\n",
        "\n",
        "# --------------------------\n",
        "# 4) DOWNLOAD RETROSHEET GAMEINFO (OFFICIAL CSV ZIP)\n",
        "# --------------------------\n",
        "zip_url = \"https://retrosheet.org/downloads/gameinfo.zip\"\n",
        "print(\"Downloading Retrosheet gameinfo.zip ...\")\n",
        "zbytes = requests.get(zip_url).content\n",
        "with zipfile.ZipFile(io.BytesIO(zbytes)) as zf:\n",
        "    with zf.open('gameinfo.csv') as f:\n",
        "        gi = pd.read_csv(f, low_memory=False)\n",
        "\n",
        "# Parse date (YYYYMMDD) and restrict to years in your data\n",
        "gi['date'] = pd.to_datetime(gi['date'].astype(str), format='%Y%m%d', errors='coerce')\n",
        "gi = gi.dropna(subset=['date'])\n",
        "gi['date'] = gi['date'].dt.date\n",
        "\n",
        "# Optional: postseason-only filter\n",
        "if POSTSEASON_ONLY and 'gametype' in gi.columns:\n",
        "    gi = gi[gi['gametype'].str.contains('post', case=False, na=False)]\n",
        "\n",
        "years = sorted(pd.DatetimeIndex(df['__date']).year.unique())\n",
        "gi = gi[gi['date'].map(lambda d: d.year).isin(years)]\n",
        "print(f\"Retrosheet rows kept for years {years}: {len(gi):,}\")\n",
        "\n",
        "# --------------------------\n",
        "# 5) BUILD OPPONENT LOOKUP (DATE, [DH #], TEAM → OPPONENT)\n",
        "# --------------------------\n",
        "# Retrosheet columns: visteam (away), hometeam (home), number (0 single / 1/2 ... DH)\n",
        "need_cols = {'visteam','hometeam','date'}\n",
        "missing_cols = need_cols - set(gi.columns)\n",
        "if missing_cols:\n",
        "    raise RuntimeError(f\"Expected columns missing in Retrosheet data: {missing_cols}. \"\n",
        "                       f\"Available: {list(gi.columns)}\")\n",
        "\n",
        "if 'number' not in gi.columns:\n",
        "    gi['number'] = 0  # fallback if absent\n",
        "\n",
        "home = gi[['date','number','hometeam','visteam']].rename(columns={'hometeam':'team','visteam':'opponent'})\n",
        "away = gi[['date','number','visteam','hometeam']].rename(columns={'visteam':'team','hometeam':'opponent'})\n",
        "opp_map = pd.concat([home, away], ignore_index=True)\n",
        "\n",
        "# Normalize codes & types\n",
        "opp_map['team'] = opp_map['team'].str.upper().str.strip()\n",
        "opp_map['opponent'] = opp_map['opponent'].str.upper().str.strip()\n",
        "opp_map['__dh'] = pd.to_numeric(opp_map['number'], errors='coerce').fillna(0).astype(int)\n",
        "opp_map = opp_map.drop(columns=['number'])\n",
        "\n",
        "# --------------------------\n",
        "# 6) MERGE (DATE + TEAM [+ DH]) → OPPONENT\n",
        "# --------------------------\n",
        "left_keys  = ['__date','__team_retro']\n",
        "right_keys = ['date','team']\n",
        "\n",
        "# Include doubleheader key if you have it\n",
        "if DH_COL is not None:\n",
        "    df['__dh'] = pd.to_numeric(df[DH_COL], errors='coerce').fillna(0).astype(int)\n",
        "    right_keys.append('__dh')\n",
        "    left_keys.append('__dh')\n",
        "else:\n",
        "    # If user doesn't have DH column, try to match without it first.\n",
        "    pass\n",
        "\n",
        "# Prepare right dataframe with (date, team[, __dh], opponent)\n",
        "right_cols = ['date','team','opponent'] + ([ '__dh' ] if '__dh' in opp_map.columns else [])\n",
        "opp_right = opp_map[right_cols]\n",
        "\n",
        "merged = df.merge(\n",
        "    opp_right,\n",
        "    left_on=left_keys,\n",
        "    right_on=right_keys,\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 7) DIAGNOSTICS\n",
        "# --------------------------\n",
        "missing = merged[merged['opponent'].isna()]\n",
        "total = len(merged)\n",
        "matched = total - len(missing)\n",
        "print(f\"\\nMatch summary: {matched:,}/{total:,} rows matched ({matched/total:.1%}).\")\n",
        "\n",
        "if not missing.empty:\n",
        "    print(\"⚠️ Some rows did not match an opponent. Showing a sample for debugging...\\n\")\n",
        "    # Show whatever columns exist among these\n",
        "    cand_cols = [DATE_COL, TEAM_COL, '__team_retro']\n",
        "    if DH_COL is not None:\n",
        "        cand_cols.append(DH_COL)\n",
        "    cand_cols += ['__date','__dh']  # helpers if present\n",
        "    show_cols = [c for c in cand_cols if c in missing.columns]\n",
        "    if show_cols:\n",
        "        display(missing[show_cols].head(20))\n",
        "    else:\n",
        "        print(\"No expected columns found in unmatched frame. Available columns:\")\n",
        "        print(list(missing.columns))\n",
        "\n",
        "    if '__team_retro' in missing.columns:\n",
        "        print(\"\\nUnmatched normalized team codes (top 20):\")\n",
        "        print(missing['__team_retro'].value_counts().head(20))\n",
        "\n",
        "# --------------------------\n",
        "# 8) FINALIZE & SAVE\n",
        "# --------------------------\n",
        "out = merged.rename(columns={'opponent':'opponent_abbr'})\n",
        "\n",
        "# Clean up helper columns\n",
        "drop_helpers = ['__date','__team_retro','__dh','date','team']  # 'date','team' only from right merge\n",
        "out = out.drop(columns=[c for c in drop_helpers if c in out.columns])\n",
        "\n",
        "out.to_csv(OUT_PATH, index=False)\n",
        "print(f\"\\n✅ Done. Saved with opponent column to: {OUT_PATH}\")\n",
        "display(out.head(10))\n",
        "\n",
        "# --- OPTIONAL: also save as Excel (.xlsx) ---\n",
        "XLSX_PATH = OUT_PATH.replace('.csv', '.xlsx')  # same name, Excel extension\n",
        "out.to_excel(XLSX_PATH, index=False)\n",
        "\n",
        "print(f\"📘 Also saved Excel version to: {XLSX_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "Owxc65hztwDy",
        "outputId": "77b32f3a-ce43-45dc-91b0-72d586f94db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: /content/mlb_playoffs_with_hfa_and_septwinpct.xlsx  shape=(664, 26)\n",
            "Detected columns → DATE_COL='date', TEAM_COL='team', DH_COL='None'\n",
            "Downloading Retrosheet gameinfo.zip ...\n",
            "Retrosheet rows kept for years [2014, 2015, 2016, 2017, 2018, 2019, 2021, 2022, 2023, 2024]: 24,677\n",
            "\n",
            "Match summary: 664/664 rows matched (100.0%).\n",
            "\n",
            "✅ Done. Saved with opponent column to: /content/mlb_playoffs_with_opponents.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   year     date_x team_x  total_teammate_mentions  total_word_count  \\\n",
              "0  2014 2014-10-02    BAL                       26              8603   \n",
              "1  2014 2014-10-02    DET                       26              6007   \n",
              "2  2014 2014-10-02    KCA                       50              9544   \n",
              "3  2014 2014-10-02    LAA                       39              8781   \n",
              "4  2014 2014-10-03    BAL                       18              8503   \n",
              "5  2014 2014-10-03    DET                       15              5582   \n",
              "6  2014 2014-10-03    KCA                       19              9993   \n",
              "7  2014 2014-10-03    LAA                       28              8299   \n",
              "8  2014 2014-10-03    LAN                       23              9464   \n",
              "9  2014 2014-10-03    SFN                       32              9238   \n",
              "\n",
              "   n_interviews team_won  is_division_series  is_championship_series  \\\n",
              "0             4      Win                   1                       0   \n",
              "1             3     Loss                   1                       0   \n",
              "2             4      Win                   1                       0   \n",
              "3             3     Loss                   1                       0   \n",
              "4             4      Win                   1                       0   \n",
              "5             3     Loss                   1                       0   \n",
              "6             4      Win                   1                       0   \n",
              "7             3     Loss                   1                       0   \n",
              "8             3     Loss                   1                       0   \n",
              "9             4      Win                   1                       0   \n",
              "\n",
              "   is_world_series  ...    OBP  Offensive WAR  Pitching WAR Defensive WAR  \\\n",
              "0                0  ...  0.311           27.3          12.9          34.2   \n",
              "1                0  ...  0.331           24.4          19.7         -45.8   \n",
              "2                0  ...  0.314           18.6          17.2          26.6   \n",
              "3                0  ...  0.322           30.8          14.5          16.0   \n",
              "4                0  ...  0.311           27.3          12.9          34.2   \n",
              "5                0  ...  0.331           24.4          19.7         -45.8   \n",
              "6                0  ...  0.314           18.6          17.2          26.6   \n",
              "7                0  ...  0.322           30.8          14.5          16.0   \n",
              "8                0  ...  0.333           32.3          19.5          24.3   \n",
              "9                0  ...  0.311           25.7           9.4          55.1   \n",
              "\n",
              "    Win%  home_field_advantage  sept_win_pct      date_y  team_y  \\\n",
              "0  0.593                     1      0.629630  2014-10-02     BAL   \n",
              "1  0.556                     0      0.615385  2014-10-02     DET   \n",
              "2  0.549                     0      0.576923  2014-10-02     KCA   \n",
              "3  0.605                     1      0.576923  2014-10-02     ANA   \n",
              "4  0.593                     1      0.629630  2014-10-03     BAL   \n",
              "5  0.556                     0      0.615385  2014-10-03     DET   \n",
              "6  0.549                     0      0.576923  2014-10-03     KCA   \n",
              "7  0.605                     1      0.576923  2014-10-03     ANA   \n",
              "8  0.580                     1      0.680000  2014-10-03     LAN   \n",
              "9  0.543                     0      0.520000  2014-10-03     SFN   \n",
              "\n",
              "   opponent_abbr  \n",
              "0            DET  \n",
              "1            BAL  \n",
              "2            ANA  \n",
              "3            KCA  \n",
              "4            DET  \n",
              "5            BAL  \n",
              "6            ANA  \n",
              "7            KCA  \n",
              "8            SLN  \n",
              "9            WAS  \n",
              "\n",
              "[10 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8143e945-b898-448c-af6e-f84bd97c8920\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>date_x</th>\n",
              "      <th>team_x</th>\n",
              "      <th>total_teammate_mentions</th>\n",
              "      <th>total_word_count</th>\n",
              "      <th>n_interviews</th>\n",
              "      <th>team_won</th>\n",
              "      <th>is_division_series</th>\n",
              "      <th>is_championship_series</th>\n",
              "      <th>is_world_series</th>\n",
              "      <th>...</th>\n",
              "      <th>OBP</th>\n",
              "      <th>Offensive WAR</th>\n",
              "      <th>Pitching WAR</th>\n",
              "      <th>Defensive WAR</th>\n",
              "      <th>Win%</th>\n",
              "      <th>home_field_advantage</th>\n",
              "      <th>sept_win_pct</th>\n",
              "      <th>date_y</th>\n",
              "      <th>team_y</th>\n",
              "      <th>opponent_abbr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-02</td>\n",
              "      <td>BAL</td>\n",
              "      <td>26</td>\n",
              "      <td>8603</td>\n",
              "      <td>4</td>\n",
              "      <td>Win</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.311</td>\n",
              "      <td>27.3</td>\n",
              "      <td>12.9</td>\n",
              "      <td>34.2</td>\n",
              "      <td>0.593</td>\n",
              "      <td>1</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>2014-10-02</td>\n",
              "      <td>BAL</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-02</td>\n",
              "      <td>DET</td>\n",
              "      <td>26</td>\n",
              "      <td>6007</td>\n",
              "      <td>3</td>\n",
              "      <td>Loss</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.331</td>\n",
              "      <td>24.4</td>\n",
              "      <td>19.7</td>\n",
              "      <td>-45.8</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>2014-10-02</td>\n",
              "      <td>DET</td>\n",
              "      <td>BAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-02</td>\n",
              "      <td>KCA</td>\n",
              "      <td>50</td>\n",
              "      <td>9544</td>\n",
              "      <td>4</td>\n",
              "      <td>Win</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.314</td>\n",
              "      <td>18.6</td>\n",
              "      <td>17.2</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.549</td>\n",
              "      <td>0</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>2014-10-02</td>\n",
              "      <td>KCA</td>\n",
              "      <td>ANA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-02</td>\n",
              "      <td>LAA</td>\n",
              "      <td>39</td>\n",
              "      <td>8781</td>\n",
              "      <td>3</td>\n",
              "      <td>Loss</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.322</td>\n",
              "      <td>30.8</td>\n",
              "      <td>14.5</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.605</td>\n",
              "      <td>1</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>2014-10-02</td>\n",
              "      <td>ANA</td>\n",
              "      <td>KCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>BAL</td>\n",
              "      <td>18</td>\n",
              "      <td>8503</td>\n",
              "      <td>4</td>\n",
              "      <td>Win</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.311</td>\n",
              "      <td>27.3</td>\n",
              "      <td>12.9</td>\n",
              "      <td>34.2</td>\n",
              "      <td>0.593</td>\n",
              "      <td>1</td>\n",
              "      <td>0.629630</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>BAL</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>DET</td>\n",
              "      <td>15</td>\n",
              "      <td>5582</td>\n",
              "      <td>3</td>\n",
              "      <td>Loss</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.331</td>\n",
              "      <td>24.4</td>\n",
              "      <td>19.7</td>\n",
              "      <td>-45.8</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>DET</td>\n",
              "      <td>BAL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>KCA</td>\n",
              "      <td>19</td>\n",
              "      <td>9993</td>\n",
              "      <td>4</td>\n",
              "      <td>Win</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.314</td>\n",
              "      <td>18.6</td>\n",
              "      <td>17.2</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.549</td>\n",
              "      <td>0</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>KCA</td>\n",
              "      <td>ANA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>LAA</td>\n",
              "      <td>28</td>\n",
              "      <td>8299</td>\n",
              "      <td>3</td>\n",
              "      <td>Loss</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.322</td>\n",
              "      <td>30.8</td>\n",
              "      <td>14.5</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.605</td>\n",
              "      <td>1</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>ANA</td>\n",
              "      <td>KCA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>LAN</td>\n",
              "      <td>23</td>\n",
              "      <td>9464</td>\n",
              "      <td>3</td>\n",
              "      <td>Loss</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.333</td>\n",
              "      <td>32.3</td>\n",
              "      <td>19.5</td>\n",
              "      <td>24.3</td>\n",
              "      <td>0.580</td>\n",
              "      <td>1</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>LAN</td>\n",
              "      <td>SLN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2014</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>SFN</td>\n",
              "      <td>32</td>\n",
              "      <td>9238</td>\n",
              "      <td>4</td>\n",
              "      <td>Win</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.311</td>\n",
              "      <td>25.7</td>\n",
              "      <td>9.4</td>\n",
              "      <td>55.1</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0</td>\n",
              "      <td>0.520000</td>\n",
              "      <td>2014-10-03</td>\n",
              "      <td>SFN</td>\n",
              "      <td>WAS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 29 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8143e945-b898-448c-af6e-f84bd97c8920')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8143e945-b898-448c-af6e-f84bd97c8920 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8143e945-b898-448c-af6e-f84bd97c8920');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f0f49ff4-4d02-46bb-8fdc-8fdeed187dc6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0f49ff4-4d02-46bb-8fdc-8fdeed187dc6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f0f49ff4-4d02-46bb-8fdc-8fdeed187dc6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Also saved Excel version to: /content/mlb_playoffs_with_opponents.xlsx\n"
          ]
        }
      ]
    }
  ]
}